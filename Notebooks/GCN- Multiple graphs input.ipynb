{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "DisjoingGraphs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "evRpH2mFxAQG"
      },
      "source": [
        "# !unzip output.zip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k69p31wtxVTP",
        "outputId": "1910e934-6930-4ae8-ee7a-59214f496ef1"
      },
      "source": [
        "\n",
        "!pip install spektral"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spektral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/02/5eb4c9e7e4b926093c127d00acdc83810b5ac77eeba32b6d5ae4eadd1da3/spektral-1.0.6-py3-none-any.whl (114kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (0.22.2.post1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.5.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.19.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->spektral) (4.4.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.12.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.4.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->spektral) (56.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.28.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.4.8)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XCcqWXrz4_z"
      },
      "source": [
        "from spektral.layers import GCNConv\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfQOPlQp7YTb"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import Dataset, DisjointLoader, Graph\n",
        "from spektral.layers import GCSConv, GlobalAvgPool\n",
        "from spektral.layers.pooling import TopKPool\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj\n",
        "import pickle as pkl\n",
        "import networkx as nx\n",
        "import pdb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD0qZF0dwSPO",
        "outputId": "11b7ff48-bfba-4d2c-ff7d-ed4ee933b6ec"
      },
      "source": [
        "################################################################################\n",
        "# PARAMETERS\n",
        "################################################################################\n",
        "learning_rate = 1e-2  # Learning rate\n",
        "epochs = 400  # Number of training epochs\n",
        "es_patience = 10  # Patience for early stopping\n",
        "batch_size = 1  # Batch size\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# LOAD DATA\n",
        "################################################################################\n",
        "import sys\n",
        "from spektral.data import Dataset, Graph\n",
        "# sys.path.append('../lib')\n",
        "# from data_pre_processing import load_data\n",
        "# sys.path.remove('../lib')\n",
        "from spektral.layers import GCNConv\n",
        "from spektral.models.gcn import GCN\n",
        "from spektral.transforms import AdjToSpTensor, LayerPreprocess\n",
        "\n",
        "def load_data(circuit_name, path_to_data=\"data\", normalize=False):\n",
        "    \"\"\"Load data.\"\"\"\n",
        "    names = [\"x\", \"y\", \"graph\"]\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(f\"{path_to_data}/{circuit_name}.{names[i]}\", \"rb\") as f:\n",
        "            if sys.version_info > (3, 0):\n",
        "                objects.append(pkl.load(f, encoding=\"latin1\"))\n",
        "            else:\n",
        "                objects.append(pkl.load(f))\n",
        "\n",
        "    x, y, graph = tuple(objects)\n",
        "\n",
        "    features = sp.csr_matrix(x).astype(int)\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph)).astype(int)\n",
        "    labels = np.array(y).reshape((-1,1))\n",
        "\n",
        "    print(adj.shape)\n",
        "    print(features.shape)\n",
        "    return adj, features, labels\n",
        "\n",
        "\n",
        "class CircuitDataset(Dataset):\n",
        "    def read(self):\n",
        "        circuits = []\n",
        "        circs = ['c6288', 'c17', 'c5315', 'c432', 'c499', 'c880', 'c1355', 'c1908', 'c3540']\n",
        "        # circs = ['c17']\n",
        "        for circ in circs:\n",
        "            A, X, labels = load_data(circ, 'output', normalize=\"\")\n",
        "            circuits.append(Graph(x=X.toarray(), a=A, y=labels))\n",
        "            print(f\"{circ}: {sum(labels)}, {len(labels)}\")\n",
        "        return circuits\n",
        "\n",
        "dataset = CircuitDataset(transforms=NormalizeAdj())\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/valid/test split\n",
        "idxs = np.random.permutation(len(dataset))\n",
        "split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\n",
        "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
        "print(idx_tr, idx_va, idx_te)\n",
        "# [8 6 5 0 1 4 2] [3] [7]\n",
        "# dataset_tr = dataset[idx_tr]\n",
        "# dataset_va = dataset[idx_va]\n",
        "# dataset_te = dataset[idx_te]\n",
        "dataset_tr = dataset[[8,6,5,1,2, 0, 3, 4, 7]]\n",
        "dataset_va = dataset[[0,3]]\n",
        "dataset_te = dataset[4,7]\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs, node_level=True)\n",
        "loader_va = DisjointLoader(dataset_va, batch_size=batch_size, node_level=True)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, node_level=True)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6288, 6288)\n",
            "(6288, 4)\n",
            "c6288: [846], 6288\n",
            "(17, 17)\n",
            "(17, 4)\n",
            "c17: [2], 17\n",
            "(5315, 5315)\n",
            "(5315, 4)\n",
            "c5315: [599], 5315\n",
            "(432, 432)\n",
            "(432, 4)\n",
            "c432: [60], 432\n",
            "(499, 499)\n",
            "(499, 4)\n",
            "c499: [50], 499\n",
            "(880, 880)\n",
            "(880, 4)\n",
            "c880: [114], 880\n",
            "(1355, 1355)\n",
            "(1355, 4)\n",
            "c1355: [192], 1355\n",
            "(1908, 1908)\n",
            "(1908, 4)\n",
            "c1908: [257], 1908\n",
            "(3540, 3540)\n",
            "(3540, 4)\n",
            "c3540: [406], 3540\n",
            "[7 1 8 2 6 3 0] [4] [5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZsZPDKRJoSf"
      },
      "source": [
        "N = sum([ g.n_nodes for g in dataset.graphs ])\n",
        "n_out = 2"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LHP2_d3y53S",
        "outputId": "f7949b1b-a126-4553-ff66-9270cf0c17fa"
      },
      "source": [
        "channels = 50          # Number of channels in the first layer\n",
        "dropout = 0.5           # Dropout rate for the features\n",
        "l2_reg = 5e-4           # L2 regularization rate\n",
        "learning_rate = 1e-2    # Learning rate\n",
        "epochs = 200            # Number of training epochs\n",
        "es_patience = 10        # Patience for early stopping\n",
        "# N = \n",
        "\n",
        "# Model definition\n",
        "X_in = Input(shape=(F, ))\n",
        "fltr_in = Input((None, ), sparse=True)\n",
        "\n",
        "dropout_1 = Dropout(dropout)(X_in)\n",
        "graph_conv_1 = GCNConv(channels,\n",
        "                         activation='relu',\n",
        "                         kernel_regularizer=l2(l2_reg),\n",
        "                         use_bias=False)([dropout_1, fltr_in])\n",
        "\n",
        "dropout_2 = Dropout(dropout)(graph_conv_1)\n",
        "graph_conv_2 = GCNConv(n_out,\n",
        "                         activation='softmax',\n",
        "                         use_bias=False)([dropout_2, fltr_in])\n",
        "\n",
        "# Build model\n",
        "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              weighted_metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='./DetectFaultInCircuit',\n",
        ")\n",
        "callback_GCN = [tbCallBack_GCN]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 4)            0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_4 (GCNConv)            (None, 50)           200         dropout_4[0][0]                  \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 50)           0           gcn_conv_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_5 (GCNConv)            (None, 2)            100         dropout_5[0][0]                  \n",
            "                                                                 input_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 300\n",
            "Trainable params: 300\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWoMzeMPdxM6"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from spektral.data import MixedLoader\n",
        "from spektral.datasets.mnist import MNIST\n",
        "from spektral.layers import GCNConv, GlobalSumPool\n",
        "from spektral.layers.ops import sp_matrix_to_sp_tensor"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdww4jrBwSPb",
        "outputId": "6f18ce66-3bce-4319-cf97-3eed1f697ba3"
      },
      "source": [
        "optimizer = Adam()\n",
        "loss_fn = SparseCategoricalCrossentropy()\n",
        "\n",
        "\n",
        "# Training function\n",
        "@tf.function\n",
        "def train_on_batch(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs[:-1], training=True)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss, acc\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(loader):\n",
        "    step = 0\n",
        "    results = []\n",
        "    for batch in loader:\n",
        "        step += 1\n",
        "        inputs, target = batch\n",
        "        predictions = model(inputs[:-1], training=False)\n",
        "        loss = loss_fn(target, predictions)\n",
        "        acc = tf.reduce_mean(sparse_categorical_accuracy(target, predictions))\n",
        "        results.append((loss, acc, len(target)))  # Keep track of batch size\n",
        "        if step == loader.steps_per_epoch:\n",
        "            results = np.array(results)\n",
        "            return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
        "\n",
        "patience = 10\n",
        "# Setup training\n",
        "best_val_loss = 99999\n",
        "current_patience = patience\n",
        "step = 0\n",
        "\n",
        "# Training loop\n",
        "results_tr = []\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "\n",
        "    # Training step\n",
        "    inputs, target = batch\n",
        "    loss, acc = train_on_batch(inputs, target)\n",
        "    results_tr.append((loss, acc, len(target)))\n",
        "\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        results_va = evaluate(loader_va)\n",
        "        if results_va[0] < best_val_loss:\n",
        "            best_val_loss = results_va[0]\n",
        "            current_patience = patience\n",
        "            results_te = evaluate(loader_te)\n",
        "        else:\n",
        "            current_patience -= 1\n",
        "            if current_patience == 0:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "        # Print results\n",
        "        results_tr = np.array(results_tr)\n",
        "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
        "        print(\n",
        "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
        "            \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
        "            \"Test loss: {:.4f}, acc: {:.4f}\".format(\n",
        "                *results_tr, *results_va, *results_te\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Reset epoch\n",
        "        results_tr = []\n",
        "        step = 0"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_on_batch at 0x7fe87315acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_on_batch at 0x7fe87315acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function train_on_batch at 0x7fe87315acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function train_on_batch at 0x7fe87315acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function train_on_batch at 0x7fe87315acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Train loss: 23.7684, acc: 0.2766 | Valid loss: 18.1345, acc: 0.1518 | Test loss: 5.5543, acc: 0.2368\n",
            "Train loss: 15.3500, acc: 0.4334 | Valid loss: 1.6430, acc: 0.8113 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 10.5897, acc: 0.6075 | Valid loss: 6.6873, acc: 0.8646 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 9.5287, acc: 0.6908 | Valid loss: 9.4304, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 9.2426, acc: 0.7388 | Valid loss: 10.7475, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 8.8542, acc: 0.7503 | Valid loss: 10.4542, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 9.1948, acc: 0.7386 | Valid loss: 9.5996, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 8.7403, acc: 0.7358 | Valid loss: 8.4715, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 8.3533, acc: 0.7211 | Valid loss: 7.8803, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 8.2028, acc: 0.7265 | Valid loss: 7.6514, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Train loss: 8.2410, acc: 0.7283 | Valid loss: 7.5467, acc: 0.8652 | Test loss: 0.6522, acc: 0.7761\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpzJ-RrNwSPe"
      },
      "source": [
        "data = dataset[4]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr69SlXd4Wzi",
        "outputId": "a6eacf2d-c21f-42a0-82b3-c22cb027369a"
      },
      "source": [
        "for batch in loader_tr:"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph(n_nodes=499, n_node_features=4, n_edge_features=None, n_labels=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbilkFE24XSD"
      },
      "source": [
        "# Evaluate model\n",
        "X_te = dataset[7].x\n",
        "A_te = dataset[7].a\n",
        "y_te = dataset[7].y\n",
        "\n",
        "y_pred = model.predict([X_te, A_te], batch_size=X_te.shape[0])\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nxKznEH4ltT",
        "outputId": "ff080119-4357-416c-c575-7e1263d02fab"
      },
      "source": [
        "np.argmax(y_pred,axis=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjHxeoeoqSbd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}