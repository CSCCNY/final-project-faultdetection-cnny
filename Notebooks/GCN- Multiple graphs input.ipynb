{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evRpH2mFxAQG",
    "outputId": "857b84d4-360c-46b2-ba32-d92eaedf3e96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open data/output.zip, data/output.zip.zip or data/output.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "# !unzip output.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k69p31wtxVTP",
    "outputId": "5f7b7761-5174-4924-8981-fe14339d7c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spektral in /usr/local/lib/python3.7/dist-packages (1.0.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (0.22.2.post1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.5)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.41.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->spektral) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2018.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.32.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.4.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.12.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.36.2)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->spektral) (56.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.28.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.10.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->spektral) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "7XCcqWXrz4_z"
   },
   "outputs": [],
   "source": [
    "from spektral.layers import GCNConv\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "KfQOPlQp7YTb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool\n",
    "from spektral.layers.pooling import TopKPool\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import pdb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qD0qZF0dwSPO",
    "outputId": "86ea698f-a279-4c5e-8652-e940d02d4eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6288, 6288)\n",
      "(6288, 4)\n",
      "c6288: [846.], 6288\n",
      "(17, 17)\n",
      "(17, 4)\n",
      "c17: [2.], 17\n",
      "(5315, 5315)\n",
      "(5315, 4)\n",
      "c5315: [599.], 5315\n",
      "(432, 432)\n",
      "(432, 4)\n",
      "c432: [60.], 432\n",
      "(499, 499)\n",
      "(499, 4)\n",
      "c499: [50.], 499\n",
      "(880, 880)\n",
      "(880, 4)\n",
      "c880: [114.], 880\n",
      "(1355, 1355)\n",
      "(1355, 4)\n",
      "c1355: [192.], 1355\n",
      "(1908, 1908)\n",
      "(1908, 4)\n",
      "c1908: [257.], 1908\n",
      "(3540, 3540)\n",
      "(3540, 4)\n",
      "c3540: [406.], 3540\n",
      "[5 4 7 6 0 2 1] [3] [8]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# PARAMETERS\n",
    "################################################################################\n",
    "learning_rate = 1e-2  # Learning rate\n",
    "epochs = 1000  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 1  # Batch size\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# LOAD DATA\n",
    "################################################################################\n",
    "import sys\n",
    "from spektral.data import Dataset, Graph\n",
    "# sys.path.append('../lib')\n",
    "# from data_pre_processing import load_data\n",
    "# sys.path.remove('../lib')\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.models.gcn import GCN\n",
    "from spektral.transforms import AdjToSpTensor, LayerPreprocess\n",
    "\n",
    "def load_data(circuit_name, path_to_data=\"data\", normalize=False):\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = [\"x\", \"y\", \"graph\"]\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(f\"{path_to_data}/{circuit_name}.{names[i]}\", \"rb\") as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding=\"latin1\"))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, graph = tuple(objects)\n",
    "\n",
    "    features = sp.csr_matrix(x).astype('float32')\n",
    "    # adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph)).astype(int)\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(graph.keys())\n",
    "    for k, v in graph.items():\n",
    "      g.add_edges_from(([(k, t) for t in v]))\n",
    "      g.add_edges_from([(k, k)])\n",
    "    adj = nx.adjacency_matrix(g)\n",
    "    labels = np.array(y).astype('float32').reshape((-1,1))\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "    return adj, features, labels\n",
    "  \n",
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "class CircuitDataset(Dataset):\n",
    "    def read(self):\n",
    "        circuits = []\n",
    "        circs = ['c6288', 'c17', 'c5315', 'c432', 'c499', 'c880', 'c1355', 'c1908', 'c3540']\n",
    "        # circs = ['c17']\n",
    "        for circ in circs:\n",
    "            A, X, labels = load_data(circ, '../data/output', normalize=\"\")\n",
    "            circuits.append(Graph(x=X.toarray(), a=A, y=labels))\n",
    "            print(f\"{circ}: {sum(labels)}, {len(labels)}\")\n",
    "        return circuits\n",
    "\n",
    "dataset = CircuitDataset(transforms=[LayerPreprocess(GCNConv)])\n",
    "\n",
    "# Parameters\n",
    "F = dataset.n_node_features  # Dimension of node features\n",
    "n_out = dataset.n_labels  # Dimension of the target\n",
    "\n",
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(dataset))\n",
    "split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "print(idx_tr, idx_va, idx_te)\n",
    "# [8 6 5 0 1 4 2] [3] [7]\n",
    "dataset_tr = dataset[idx_tr]\n",
    "dataset_va = dataset[idx_va]\n",
    "dataset_te = dataset[idx_te]\n",
    "# dataset_tr = dataset[[8,6,5,1,2, 0, 3, 4, 7]]\n",
    "# dataset_va = dataset[[0,3]]\n",
    "# dataset_te = dataset[4,7]\n",
    "# loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs, node_level=True)\n",
    "# loader_va = DisjointLoader(dataset_va, batch_size=batch_size, node_level=True)\n",
    "# loader_te = DisjointLoader(dataset_te, batch_size=batch_size, node_level=True)\n",
    "def load_tr_data(epochs=400):\n",
    "    loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs, node_level=True)\n",
    "    loader_va = DisjointLoader(dataset_va, batch_size=batch_size, node_level=True)\n",
    "    loader_te = DisjointLoader(dataset_te, batch_size=batch_size, node_level=True)\n",
    "    return loader_tr, loader_va, loader_te\n",
    "\n",
    "loader_tr, loader_va, loader_te = load_tr_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "-Hiec2Jh-uFv"
   },
   "outputs": [],
   "source": [
    "# loader_tr = DisjointLoader(dataset_tr, batch_size=dataset_tr.n_graphs, epochs=epochs, node_level=True)\n",
    "# batch = loader_tr.__next__()\n",
    "# inputs, target = batch\n",
    "# x, a, i = inputs\n",
    "# print(x.shape, a.shape, target.shape)\n",
    "# print(x.dtype, a.dtype, target.dtype)\n",
    "# sum([ g.n_nodes for g in dataset_tr ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wn_SYZAirrvq",
    "outputId": "5d0877bf-7e38-46ac-f7f0-4ff8ceeb95b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=432, n_node_features=4, n_edge_features=None, n_labels=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZsZPDKRJoSf",
    "outputId": "55cb3423-0f12-460c-9e2c-0e97489bb48c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20234"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = sum([ g.n_nodes for g in dataset.graphs ])\n",
    "n_out = 1\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LHP2_d3y53S",
    "outputId": "e5bd601b-6b66-4c74-f6ed-895dd617e40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_in:  (None, 4)\n",
      "drp 1 KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name=None), name='dropout_28/Identity:0', description=\"created by layer 'dropout_28'\")\n",
      "graph conv 1 KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name=None), name='gcn_conv_40/Relu:0', description=\"created by layer 'gcn_conv_40'\")\n",
      "drp 2 KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name=None), name='dropout_29/Identity:0', description=\"created by layer 'dropout_29'\")\n",
      "graph conv 2 KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='gcn_conv_41/Sigmoid:0', description=\"created by layer 'gcn_conv_41'\")\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 4)            0           input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gcn_conv_40 (GCNConv)           (None, 50)           200         dropout_28[0][0]                 \n",
      "                                                                 input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 50)           0           gcn_conv_40[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gcn_conv_41 (GCNConv)           (None, 1)            50          dropout_29[0][0]                 \n",
      "                                                                 input_38[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "channels = 50          # Number of channels in the first layer\n",
    "dropout = 0.1           # Dropout rate for the features\n",
    "l2_reg = 5e-4           # L2 regularization rate\n",
    "learning_rate = 1e-2    # Learning rate\n",
    "epochs = 1000            # Number of training epochs\n",
    "es_patience = 10        # Patience for early stopping\n",
    "# N = \n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, ))\n",
    "print(\"X_in: \", X_in.shape)\n",
    "fltr_in = Input((None, ), sparse=True)\n",
    "# mask_in = Input((None, ))\n",
    "\n",
    "dropout_1 = Dropout(dropout)(X_in)\n",
    "print(\"drp 1\", dropout_1)\n",
    "graph_conv_1 = GCNConv(channels,\n",
    "                         activation='relu',\n",
    "                         kernel_regularizer=l2(l2_reg),\n",
    "                         use_bias=False)([dropout_1, fltr_in])\n",
    "print(\"graph conv 1\", graph_conv_1)\n",
    "dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "print(\"drp 2\", dropout_2)\n",
    "# graph_conv_2 = GCNConv(channels,\n",
    "#                          activation='relu',\n",
    "#                          kernel_regularizer=l2(l2_reg),\n",
    "#                          use_bias=False)([dropout_2, fltr_in])\n",
    "# dropout_3 = Dropout(dropout)(graph_conv_1)\n",
    "# print(\"drp 2\", dropout_2)\n",
    "graph_conv_3 = GCNConv(n_out,\n",
    "                         activation='sigmoid',\n",
    "                         use_bias=False)([dropout_2, fltr_in])\n",
    "print(\"graph conv 2\", graph_conv_3)\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_3)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               weighted_metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./DetectFaultInCircuit',\n",
    ")\n",
    "callback_GCN = [tbCallBack_GCN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_in:  (None, 4)\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4)            16          input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 4)            0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gcn_conv_51 (GCNConv)           (None, 50)           250         dropout_36[0][0]                 \n",
      "                                                                 input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 50)           200         gcn_conv_51[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 50)           0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gcn_conv_52 (GCNConv)           (None, 1)            51          dropout_37[0][0]                 \n",
      "                                                                 input_46[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 517\n",
      "Trainable params: 409\n",
      "Non-trainable params: 108\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "channels = 50          # Number of channels in the first layer\n",
    "dropout = 0.05         # Dropout rate for the features\n",
    "l2_reg = 5e-4           # L2 regularization rate\n",
    "learning_rate = 1e-2    # Learning rate\n",
    "epochs = 1000            # Number of training epochs\n",
    "es_patience = 10        # Patience for early stopping\n",
    "# N = \n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, ))\n",
    "print(\"X_in: \", X_in.shape)\n",
    "fltr_in = Input((None, ), sparse=True)\n",
    "# mask_in = Input((None, ))\n",
    "\n",
    "x_1 = BatchNormalization()(X_in)\n",
    "dropout_1 = Dropout(dropout)(x_1)\n",
    "graph_conv_1 = GCNConv(channels,\n",
    "                         activation='relu',\n",
    "                         use_bias=True)([dropout_1, fltr_in])\n",
    "x_2 = BatchNormalization()(graph_conv_1)\n",
    "dropout_2 = Dropout(dropout)(x_2)\n",
    "graph_conv_3 = GCNConv(n_out,\n",
    "                         activation='sigmoid',\n",
    "                         use_bias=True)([dropout_2, fltr_in])\n",
    "\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_3)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='categorical_crossentropy',\n",
    "#               weighted_metrics=['acc'])\n",
    "\n",
    "# x_in = Input(shape=(F,))\n",
    "# a_in = Input((None, ), sparse=True)\n",
    "# x_1 = GCNConv(channels, activation=\"relu\")([x_in, a_in])\n",
    "# x_1 = BatchNormalization()(x_1)\n",
    "# x_1 = Dropout(dropout)(x_1)\n",
    "# x_2 = GCNConv(channels, activation=\"relu\")([x_1, a_in])\n",
    "# x_2 = BatchNormalization()(x_2)\n",
    "# x_2 = Dropout(dropout)(x_2)\n",
    "# x_3 = GCNConv(n_out, activation=\"sigmoid\")([x_2, a_in])\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mse',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./DetectFaultInCircuit',\n",
    ")\n",
    "callback_GCN = [tbCallBack_GCN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "wWoMzeMPdxM6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy, binary_accuracy, binary_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalSumPool\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdww4jrBwSPb",
    "outputId": "77782094-c8aa-4e6c-e71c-84422d1c99a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5253 calls to <function train_on_batch at 0x7fd5ba5dde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 5254 calls to <function train_on_batch at 0x7fd5ba5dde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 5255 calls to <function train_on_batch at 0x7fd5ba5dde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 5256 calls to <function train_on_batch at 0x7fd5ba5dde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Train loss: 0.2065, acc: 0.8602 | Valid loss: 8.4295, acc: 0.8611 | Test loss: 0.8567, acc: 0.9565\n",
      "Train loss: 0.2039, acc: 0.8558 | Valid loss: 8.4249, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2092, acc: 0.8600 | Valid loss: 8.5052, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2029, acc: 0.8592 | Valid loss: 8.5764, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2034, acc: 0.8549 | Valid loss: 8.6335, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2052, acc: 0.8607 | Valid loss: 8.6217, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2025, acc: 0.8568 | Valid loss: 8.6599, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2060, acc: 0.8524 | Valid loss: 8.7579, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1991, acc: 0.8548 | Valid loss: 8.8248, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1998, acc: 0.8573 | Valid loss: 8.8460, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2097, acc: 0.8552 | Valid loss: 8.8362, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Early stopping\n",
      "Train loss: 0.1901, acc: 0.8611 | Valid loss: 8.8396, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8631 | Valid loss: 8.8184, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2041, acc: 0.8631 | Valid loss: 8.7734, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1935, acc: 0.8635 | Valid loss: 8.7667, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8615 | Valid loss: 8.7939, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2023, acc: 0.8612 | Valid loss: 8.8063, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2091, acc: 0.8620 | Valid loss: 8.8199, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8605 | Valid loss: 8.7810, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2032, acc: 0.8554 | Valid loss: 8.7817, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2001, acc: 0.8567 | Valid loss: 8.7825, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8580 | Valid loss: 8.8085, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8597 | Valid loss: 8.8799, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8615 | Valid loss: 8.9144, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1998, acc: 0.8627 | Valid loss: 8.9299, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2021, acc: 0.8621 | Valid loss: 8.9735, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1983, acc: 0.8619 | Valid loss: 8.9844, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2041, acc: 0.8584 | Valid loss: 9.0125, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2048, acc: 0.8533 | Valid loss: 8.9625, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2034, acc: 0.8574 | Valid loss: 8.9157, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1913, acc: 0.8605 | Valid loss: 8.8931, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8610 | Valid loss: 8.9186, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1971, acc: 0.8648 | Valid loss: 8.9545, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1893, acc: 0.8650 | Valid loss: 8.9947, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1892, acc: 0.8670 | Valid loss: 8.9751, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8653 | Valid loss: 8.9705, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2017, acc: 0.8640 | Valid loss: 9.0332, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2048, acc: 0.8568 | Valid loss: 8.9835, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2223, acc: 0.8564 | Valid loss: 8.9156, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2150, acc: 0.8575 | Valid loss: 8.9102, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2041, acc: 0.8555 | Valid loss: 9.0072, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2046, acc: 0.8557 | Valid loss: 9.0400, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8570 | Valid loss: 9.0335, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2032, acc: 0.8570 | Valid loss: 9.0181, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8653 | Valid loss: 9.0149, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1933, acc: 0.8657 | Valid loss: 9.0234, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8642 | Valid loss: 9.0101, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2152, acc: 0.8547 | Valid loss: 9.0965, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2158, acc: 0.8537 | Valid loss: 9.1282, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2194, acc: 0.8470 | Valid loss: 9.1480, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2097, acc: 0.8470 | Valid loss: 9.1117, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2093, acc: 0.8511 | Valid loss: 9.0777, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2046, acc: 0.8548 | Valid loss: 9.0746, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8585 | Valid loss: 9.0809, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2025, acc: 0.8595 | Valid loss: 9.0451, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2057, acc: 0.8587 | Valid loss: 9.0067, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1997, acc: 0.8627 | Valid loss: 8.9878, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2082, acc: 0.8567 | Valid loss: 8.9943, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1978, acc: 0.8572 | Valid loss: 9.0334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2088, acc: 0.8566 | Valid loss: 9.0348, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2022, acc: 0.8561 | Valid loss: 9.0468, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8564 | Valid loss: 9.0610, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1960, acc: 0.8596 | Valid loss: 9.0816, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1992, acc: 0.8616 | Valid loss: 9.0944, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8624 | Valid loss: 9.1011, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8643 | Valid loss: 9.1010, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8671 | Valid loss: 9.1086, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2102, acc: 0.8627 | Valid loss: 9.1413, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1991, acc: 0.8627 | Valid loss: 9.1405, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1979, acc: 0.8614 | Valid loss: 9.1140, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2024, acc: 0.8602 | Valid loss: 9.0755, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2138, acc: 0.8564 | Valid loss: 8.9334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2174, acc: 0.8484 | Valid loss: 8.8354, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2153, acc: 0.8516 | Valid loss: 8.7324, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2068, acc: 0.8512 | Valid loss: 8.6569, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2058, acc: 0.8527 | Valid loss: 8.6334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2059, acc: 0.8597 | Valid loss: 8.6882, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2123, acc: 0.8594 | Valid loss: 8.8000, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8603 | Valid loss: 8.9108, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2057, acc: 0.8540 | Valid loss: 9.0802, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2076, acc: 0.8544 | Valid loss: 9.1206, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2067, acc: 0.8514 | Valid loss: 9.1096, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1995, acc: 0.8524 | Valid loss: 9.0963, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2093, acc: 0.8520 | Valid loss: 9.0818, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8565 | Valid loss: 9.0637, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1942, acc: 0.8618 | Valid loss: 9.0136, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1960, acc: 0.8618 | Valid loss: 8.9907, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1945, acc: 0.8624 | Valid loss: 9.0020, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8627 | Valid loss: 8.9967, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8637 | Valid loss: 9.0025, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1975, acc: 0.8638 | Valid loss: 8.9755, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1949, acc: 0.8643 | Valid loss: 8.9746, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2023, acc: 0.8605 | Valid loss: 9.0078, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2054, acc: 0.8637 | Valid loss: 9.0381, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1945, acc: 0.8620 | Valid loss: 9.0623, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2043, acc: 0.8535 | Valid loss: 9.0705, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2033, acc: 0.8514 | Valid loss: 9.0406, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8569 | Valid loss: 8.9828, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2174, acc: 0.8541 | Valid loss: 8.9621, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2140, acc: 0.8503 | Valid loss: 8.9018, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2122, acc: 0.8519 | Valid loss: 8.9214, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2117, acc: 0.8559 | Valid loss: 8.9547, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2053, acc: 0.8530 | Valid loss: 9.0004, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8582 | Valid loss: 9.0126, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1976, acc: 0.8615 | Valid loss: 9.0264, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8648 | Valid loss: 9.0164, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8617 | Valid loss: 9.0007, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1968, acc: 0.8598 | Valid loss: 9.0163, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1974, acc: 0.8661 | Valid loss: 8.8917, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2024, acc: 0.8626 | Valid loss: 8.8897, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2120, acc: 0.8562 | Valid loss: 8.9018, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2185, acc: 0.8507 | Valid loss: 8.8934, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2280, acc: 0.8484 | Valid loss: 8.8592, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2139, acc: 0.8500 | Valid loss: 8.8488, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2120, acc: 0.8463 | Valid loss: 8.8620, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2032, acc: 0.8521 | Valid loss: 8.9345, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2026, acc: 0.8543 | Valid loss: 9.0207, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2063, acc: 0.8531 | Valid loss: 9.0791, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2016, acc: 0.8593 | Valid loss: 9.1060, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1983, acc: 0.8552 | Valid loss: 9.1627, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1976, acc: 0.8610 | Valid loss: 9.2273, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8608 | Valid loss: 9.2572, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2029, acc: 0.8600 | Valid loss: 9.2710, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2051, acc: 0.8588 | Valid loss: 9.1623, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8649 | Valid loss: 9.1073, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8628 | Valid loss: 9.1378, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2052, acc: 0.8548 | Valid loss: 9.0365, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2169, acc: 0.8535 | Valid loss: 9.0256, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2033, acc: 0.8540 | Valid loss: 9.0521, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1998, acc: 0.8560 | Valid loss: 9.0223, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2132, acc: 0.8518 | Valid loss: 8.9137, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8621 | Valid loss: 8.8576, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2108, acc: 0.8580 | Valid loss: 8.8502, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2069, acc: 0.8562 | Valid loss: 8.9367, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1984, acc: 0.8546 | Valid loss: 9.0317, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8553 | Valid loss: 9.1368, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8539 | Valid loss: 9.2363, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2026, acc: 0.8565 | Valid loss: 9.2493, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2041, acc: 0.8588 | Valid loss: 9.2409, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1995, acc: 0.8631 | Valid loss: 9.2512, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2056, acc: 0.8597 | Valid loss: 9.2859, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2051, acc: 0.8587 | Valid loss: 9.3048, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1978, acc: 0.8559 | Valid loss: 9.3042, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2028, acc: 0.8578 | Valid loss: 9.2978, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2033, acc: 0.8647 | Valid loss: 9.3217, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2044, acc: 0.8607 | Valid loss: 9.2446, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1967, acc: 0.8608 | Valid loss: 9.1610, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2004, acc: 0.8623 | Valid loss: 9.1662, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1912, acc: 0.8626 | Valid loss: 9.1941, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2058, acc: 0.8622 | Valid loss: 9.2494, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1974, acc: 0.8611 | Valid loss: 9.2846, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8623 | Valid loss: 9.3097, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8615 | Valid loss: 9.3186, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2030, acc: 0.8605 | Valid loss: 9.2610, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8629 | Valid loss: 9.2286, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8602 | Valid loss: 9.2380, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1975, acc: 0.8611 | Valid loss: 9.2651, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2063, acc: 0.8590 | Valid loss: 9.3075, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1969, acc: 0.8619 | Valid loss: 9.3449, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1976, acc: 0.8590 | Valid loss: 9.3710, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1997, acc: 0.8593 | Valid loss: 9.4146, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1935, acc: 0.8610 | Valid loss: 9.4077, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2028, acc: 0.8572 | Valid loss: 9.3893, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2044, acc: 0.8528 | Valid loss: 9.3871, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2054, acc: 0.8530 | Valid loss: 9.3604, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1978, acc: 0.8544 | Valid loss: 9.3139, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2087, acc: 0.8570 | Valid loss: 9.2837, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1921, acc: 0.8601 | Valid loss: 9.3001, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1983, acc: 0.8612 | Valid loss: 9.3294, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2077, acc: 0.8528 | Valid loss: 9.4400, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2088, acc: 0.8480 | Valid loss: 9.4128, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2126, acc: 0.8498 | Valid loss: 9.2785, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2099, acc: 0.8551 | Valid loss: 9.1742, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2115, acc: 0.8602 | Valid loss: 9.1126, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2023, acc: 0.8588 | Valid loss: 9.0914, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1981, acc: 0.8597 | Valid loss: 9.0817, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8597 | Valid loss: 9.0770, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8607 | Valid loss: 9.0697, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8599 | Valid loss: 9.0816, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8618 | Valid loss: 9.0427, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8651 | Valid loss: 9.0255, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2090, acc: 0.8629 | Valid loss: 9.1169, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8625 | Valid loss: 9.1932, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1898, acc: 0.8598 | Valid loss: 9.2459, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2077, acc: 0.8568 | Valid loss: 9.2821, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1997, acc: 0.8592 | Valid loss: 9.2733, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8611 | Valid loss: 9.1834, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2110, acc: 0.8593 | Valid loss: 9.2122, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8652 | Valid loss: 9.2878, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2007, acc: 0.8659 | Valid loss: 9.3557, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2000, acc: 0.8595 | Valid loss: 9.3933, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1969, acc: 0.8573 | Valid loss: 9.4454, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8580 | Valid loss: 9.4590, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8588 | Valid loss: 9.4665, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8622 | Valid loss: 9.4536, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8619 | Valid loss: 9.4728, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1977, acc: 0.8607 | Valid loss: 9.5093, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8643 | Valid loss: 9.5301, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8642 | Valid loss: 9.5281, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8610 | Valid loss: 9.4738, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8634 | Valid loss: 9.4450, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8651 | Valid loss: 9.4183, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1966, acc: 0.8633 | Valid loss: 9.4079, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8628 | Valid loss: 9.3964, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1917, acc: 0.8618 | Valid loss: 9.3105, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2198, acc: 0.8565 | Valid loss: 9.0968, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2104, acc: 0.8571 | Valid loss: 9.0290, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2099, acc: 0.8568 | Valid loss: 9.0888, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2014, acc: 0.8489 | Valid loss: 9.1013, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2070, acc: 0.8561 | Valid loss: 9.1313, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2014, acc: 0.8561 | Valid loss: 9.1553, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2003, acc: 0.8601 | Valid loss: 9.1753, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1928, acc: 0.8621 | Valid loss: 9.2338, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2015, acc: 0.8572 | Valid loss: 9.2893, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2053, acc: 0.8630 | Valid loss: 9.3438, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1954, acc: 0.8603 | Valid loss: 9.4024, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2004, acc: 0.8580 | Valid loss: 9.3944, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1998, acc: 0.8605 | Valid loss: 9.4315, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1988, acc: 0.8599 | Valid loss: 9.4635, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2031, acc: 0.8596 | Valid loss: 9.4903, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8604 | Valid loss: 9.5440, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2026, acc: 0.8653 | Valid loss: 9.5383, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1866, acc: 0.8628 | Valid loss: 9.5029, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1961, acc: 0.8660 | Valid loss: 9.5130, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1979, acc: 0.8649 | Valid loss: 9.5985, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8663 | Valid loss: 9.6291, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1838, acc: 0.8644 | Valid loss: 9.5344, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1873, acc: 0.8680 | Valid loss: 9.4541, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8667 | Valid loss: 9.4935, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1925, acc: 0.8684 | Valid loss: 9.5716, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1931, acc: 0.8669 | Valid loss: 9.6620, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8629 | Valid loss: 9.7113, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1898, acc: 0.8626 | Valid loss: 9.7380, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8640 | Valid loss: 9.7173, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2021, acc: 0.8646 | Valid loss: 9.7007, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1827, acc: 0.8700 | Valid loss: 9.7146, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8659 | Valid loss: 9.6979, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1900, acc: 0.8700 | Valid loss: 9.6823, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8672 | Valid loss: 9.6609, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1931, acc: 0.8696 | Valid loss: 9.6524, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1925, acc: 0.8677 | Valid loss: 9.6724, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8682 | Valid loss: 9.6539, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8653 | Valid loss: 9.6436, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1976, acc: 0.8656 | Valid loss: 9.6747, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8638 | Valid loss: 9.6301, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2036, acc: 0.8640 | Valid loss: 9.5734, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1895, acc: 0.8629 | Valid loss: 9.5094, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1971, acc: 0.8641 | Valid loss: 9.4957, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2048, acc: 0.8653 | Valid loss: 9.5334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8607 | Valid loss: 9.5199, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1954, acc: 0.8641 | Valid loss: 9.4686, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8600 | Valid loss: 9.4047, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8626 | Valid loss: 9.4071, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8659 | Valid loss: 9.4676, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1991, acc: 0.8666 | Valid loss: 9.5301, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1917, acc: 0.8687 | Valid loss: 9.5690, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1863, acc: 0.8689 | Valid loss: 9.5699, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8660 | Valid loss: 9.6150, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1913, acc: 0.8688 | Valid loss: 9.6469, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8693 | Valid loss: 9.6256, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1995, acc: 0.8617 | Valid loss: 9.6251, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1951, acc: 0.8640 | Valid loss: 9.6290, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8661 | Valid loss: 9.6335, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8642 | Valid loss: 9.6106, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1882, acc: 0.8679 | Valid loss: 9.6449, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1945, acc: 0.8672 | Valid loss: 9.6562, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1871, acc: 0.8640 | Valid loss: 9.6182, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2021, acc: 0.8656 | Valid loss: 9.6114, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2020, acc: 0.8658 | Valid loss: 9.6743, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8683 | Valid loss: 9.7626, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8701 | Valid loss: 9.7945, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8679 | Valid loss: 9.7920, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1933, acc: 0.8666 | Valid loss: 9.7501, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8647 | Valid loss: 9.7640, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1964, acc: 0.8641 | Valid loss: 9.7533, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8670 | Valid loss: 9.7168, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1865, acc: 0.8678 | Valid loss: 9.5896, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8723 | Valid loss: 9.5634, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1961, acc: 0.8708 | Valid loss: 9.6508, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1872, acc: 0.8648 | Valid loss: 9.8238, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1975, acc: 0.8610 | Valid loss: 9.8769, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8652 | Valid loss: 9.8457, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1933, acc: 0.8693 | Valid loss: 9.7750, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8695 | Valid loss: 9.7014, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8691 | Valid loss: 9.5754, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1997, acc: 0.8652 | Valid loss: 9.5851, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1876, acc: 0.8659 | Valid loss: 9.6198, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8658 | Valid loss: 9.6530, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1928, acc: 0.8658 | Valid loss: 9.6916, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8651 | Valid loss: 9.7497, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8655 | Valid loss: 9.7916, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8711 | Valid loss: 9.8614, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1996, acc: 0.8658 | Valid loss: 9.9214, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1931, acc: 0.8645 | Valid loss: 9.9342, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1912, acc: 0.8620 | Valid loss: 9.8889, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8644 | Valid loss: 9.8774, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1857, acc: 0.8672 | Valid loss: 9.8947, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8701 | Valid loss: 9.8947, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8717 | Valid loss: 9.8970, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2064, acc: 0.8673 | Valid loss: 9.8903, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1864, acc: 0.8624 | Valid loss: 9.9333, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8604 | Valid loss: 9.9124, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1988, acc: 0.8627 | Valid loss: 9.8763, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8696 | Valid loss: 9.8647, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2001, acc: 0.8688 | Valid loss: 9.8111, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2002, acc: 0.8693 | Valid loss: 9.6931, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2005, acc: 0.8698 | Valid loss: 9.6671, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8666 | Valid loss: 9.7002, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8653 | Valid loss: 9.6793, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1866, acc: 0.8648 | Valid loss: 9.6741, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8671 | Valid loss: 9.7224, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8699 | Valid loss: 9.7572, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1840, acc: 0.8706 | Valid loss: 9.7770, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8697 | Valid loss: 9.8454, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1992, acc: 0.8664 | Valid loss: 9.9032, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1831, acc: 0.8677 | Valid loss: 9.9361, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1894, acc: 0.8692 | Valid loss: 9.9328, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8655 | Valid loss: 9.9662, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8703 | Valid loss: 9.9191, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1871, acc: 0.8706 | Valid loss: 9.9087, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1986, acc: 0.8677 | Valid loss: 9.9569, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1935, acc: 0.8675 | Valid loss: 10.0449, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1967, acc: 0.8636 | Valid loss: 10.1004, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8630 | Valid loss: 10.0746, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1871, acc: 0.8675 | Valid loss: 10.0186, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8695 | Valid loss: 9.9744, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1839, acc: 0.8707 | Valid loss: 9.9509, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1834, acc: 0.8689 | Valid loss: 9.9198, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1811, acc: 0.8736 | Valid loss: 9.9140, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1884, acc: 0.8722 | Valid loss: 9.9740, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1914, acc: 0.8690 | Valid loss: 10.0313, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8674 | Valid loss: 10.0970, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1942, acc: 0.8643 | Valid loss: 10.1002, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8580 | Valid loss: 10.0254, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1912, acc: 0.8643 | Valid loss: 9.9960, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1981, acc: 0.8656 | Valid loss: 9.9375, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8674 | Valid loss: 9.7407, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2041, acc: 0.8649 | Valid loss: 9.5824, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2261, acc: 0.8568 | Valid loss: 9.2751, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2418, acc: 0.8494 | Valid loss: 9.2283, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2109, acc: 0.8429 | Valid loss: 9.2504, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2069, acc: 0.8489 | Valid loss: 9.2709, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2038, acc: 0.8591 | Valid loss: 9.2886, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2047, acc: 0.8585 | Valid loss: 9.3547, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2022, acc: 0.8584 | Valid loss: 9.4212, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8590 | Valid loss: 9.3512, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2117, acc: 0.8610 | Valid loss: 9.2744, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8648 | Valid loss: 9.2827, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2016, acc: 0.8610 | Valid loss: 9.4164, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1976, acc: 0.8578 | Valid loss: 9.6304, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1945, acc: 0.8561 | Valid loss: 9.7318, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8603 | Valid loss: 9.7282, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1947, acc: 0.8629 | Valid loss: 9.6863, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2050, acc: 0.8635 | Valid loss: 9.6657, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8655 | Valid loss: 9.6738, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8664 | Valid loss: 9.7134, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1960, acc: 0.8654 | Valid loss: 9.7256, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8639 | Valid loss: 9.7118, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8671 | Valid loss: 9.7685, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1872, acc: 0.8688 | Valid loss: 9.7716, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8661 | Valid loss: 9.8156, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1844, acc: 0.8667 | Valid loss: 9.8502, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1894, acc: 0.8688 | Valid loss: 9.8829, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8707 | Valid loss: 9.9566, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1971, acc: 0.8709 | Valid loss: 10.0229, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8610 | Valid loss: 10.1125, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8581 | Valid loss: 10.0171, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2056, acc: 0.8586 | Valid loss: 9.7697, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1920, acc: 0.8667 | Valid loss: 9.6988, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1861, acc: 0.8702 | Valid loss: 9.6911, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8731 | Valid loss: 9.7183, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1923, acc: 0.8666 | Valid loss: 9.7081, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1921, acc: 0.8665 | Valid loss: 9.7538, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8639 | Valid loss: 9.8549, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8668 | Valid loss: 9.9362, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1946, acc: 0.8671 | Valid loss: 10.0143, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8691 | Valid loss: 10.0604, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8643 | Valid loss: 10.0724, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1837, acc: 0.8639 | Valid loss: 10.0807, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1817, acc: 0.8687 | Valid loss: 10.1231, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1885, acc: 0.8713 | Valid loss: 10.1574, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8711 | Valid loss: 10.2392, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1840, acc: 0.8691 | Valid loss: 10.2863, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8684 | Valid loss: 10.2959, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1893, acc: 0.8669 | Valid loss: 10.2764, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8629 | Valid loss: 10.2245, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8660 | Valid loss: 10.1922, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1850, acc: 0.8688 | Valid loss: 10.1962, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1848, acc: 0.8695 | Valid loss: 10.1823, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1845, acc: 0.8711 | Valid loss: 10.1933, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8699 | Valid loss: 10.2271, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1942, acc: 0.8704 | Valid loss: 10.2640, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1874, acc: 0.8728 | Valid loss: 10.2992, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1834, acc: 0.8729 | Valid loss: 10.3316, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1801, acc: 0.8739 | Valid loss: 10.3455, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8718 | Valid loss: 10.4463, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1907, acc: 0.8662 | Valid loss: 10.5413, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2006, acc: 0.8647 | Valid loss: 10.5390, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8662 | Valid loss: 10.4588, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1831, acc: 0.8680 | Valid loss: 10.3896, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8670 | Valid loss: 10.3064, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2176, acc: 0.8692 | Valid loss: 10.2687, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1855, acc: 0.8702 | Valid loss: 10.2967, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2071, acc: 0.8679 | Valid loss: 10.2276, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1942, acc: 0.8704 | Valid loss: 10.2294, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8696 | Valid loss: 10.3137, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1862, acc: 0.8653 | Valid loss: 10.2960, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8703 | Valid loss: 10.2439, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1927, acc: 0.8679 | Valid loss: 10.2721, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1885, acc: 0.8672 | Valid loss: 10.2837, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8668 | Valid loss: 10.2882, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1915, acc: 0.8656 | Valid loss: 10.3043, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8683 | Valid loss: 10.3525, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1865, acc: 0.8709 | Valid loss: 10.3614, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1887, acc: 0.8673 | Valid loss: 10.3664, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1808, acc: 0.8698 | Valid loss: 10.3665, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8675 | Valid loss: 10.3994, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1875, acc: 0.8673 | Valid loss: 10.4265, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2128, acc: 0.8605 | Valid loss: 10.3712, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2172, acc: 0.8572 | Valid loss: 10.3499, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2154, acc: 0.8579 | Valid loss: 10.3731, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1996, acc: 0.8637 | Valid loss: 10.4117, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8667 | Valid loss: 10.4463, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8657 | Valid loss: 10.4989, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8648 | Valid loss: 10.5233, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1987, acc: 0.8666 | Valid loss: 10.4221, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1969, acc: 0.8659 | Valid loss: 10.4256, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8676 | Valid loss: 10.3164, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1926, acc: 0.8673 | Valid loss: 10.3164, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1940, acc: 0.8664 | Valid loss: 10.2889, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8666 | Valid loss: 10.2627, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8641 | Valid loss: 10.3214, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1857, acc: 0.8696 | Valid loss: 10.4111, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8674 | Valid loss: 10.5303, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2084, acc: 0.8683 | Valid loss: 10.5886, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8679 | Valid loss: 10.6041, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1874, acc: 0.8645 | Valid loss: 10.6142, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1848, acc: 0.8661 | Valid loss: 10.5498, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1849, acc: 0.8683 | Valid loss: 10.4837, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1900, acc: 0.8701 | Valid loss: 10.4381, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1831, acc: 0.8740 | Valid loss: 10.4315, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1857, acc: 0.8741 | Valid loss: 10.5048, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1851, acc: 0.8712 | Valid loss: 10.5543, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1842, acc: 0.8692 | Valid loss: 10.5888, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1854, acc: 0.8695 | Valid loss: 10.5649, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8728 | Valid loss: 10.5976, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8703 | Valid loss: 10.5876, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8589 | Valid loss: 10.6309, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1984, acc: 0.8561 | Valid loss: 10.5238, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8624 | Valid loss: 10.4586, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8669 | Valid loss: 10.2956, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8698 | Valid loss: 10.2022, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8685 | Valid loss: 10.2016, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1913, acc: 0.8680 | Valid loss: 10.2831, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1890, acc: 0.8626 | Valid loss: 10.3985, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1960, acc: 0.8671 | Valid loss: 10.3422, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8651 | Valid loss: 10.3217, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1893, acc: 0.8662 | Valid loss: 10.3931, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8685 | Valid loss: 10.4409, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2014, acc: 0.8632 | Valid loss: 10.4069, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1825, acc: 0.8675 | Valid loss: 10.3838, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1951, acc: 0.8651 | Valid loss: 10.3463, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2013, acc: 0.8641 | Valid loss: 10.3172, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1985, acc: 0.8649 | Valid loss: 10.3220, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8618 | Valid loss: 10.2828, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8656 | Valid loss: 10.1644, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8674 | Valid loss: 10.1480, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2054, acc: 0.8674 | Valid loss: 10.1872, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1877, acc: 0.8675 | Valid loss: 10.2673, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1990, acc: 0.8696 | Valid loss: 10.1970, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8662 | Valid loss: 10.2718, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2155, acc: 0.8506 | Valid loss: 10.2645, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1991, acc: 0.8524 | Valid loss: 10.1673, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1977, acc: 0.8569 | Valid loss: 10.0932, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1989, acc: 0.8629 | Valid loss: 10.0494, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8658 | Valid loss: 9.7918, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2206, acc: 0.8508 | Valid loss: 9.5461, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2185, acc: 0.8475 | Valid loss: 9.3135, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2194, acc: 0.8471 | Valid loss: 9.1593, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2021, acc: 0.8519 | Valid loss: 9.0621, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2002, acc: 0.8574 | Valid loss: 9.0715, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8620 | Valid loss: 9.1614, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2053, acc: 0.8619 | Valid loss: 9.3268, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8642 | Valid loss: 9.4508, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1931, acc: 0.8623 | Valid loss: 9.6041, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1967, acc: 0.8643 | Valid loss: 9.7297, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1987, acc: 0.8624 | Valid loss: 9.8275, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8616 | Valid loss: 9.8784, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8650 | Valid loss: 9.9041, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8640 | Valid loss: 9.9097, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1932, acc: 0.8659 | Valid loss: 9.9727, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1967, acc: 0.8610 | Valid loss: 9.9719, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8592 | Valid loss: 9.9870, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8619 | Valid loss: 9.9738, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8672 | Valid loss: 9.9224, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1990, acc: 0.8676 | Valid loss: 9.9258, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8690 | Valid loss: 10.0683, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1880, acc: 0.8686 | Valid loss: 10.0958, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8672 | Valid loss: 10.1369, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1931, acc: 0.8639 | Valid loss: 10.1637, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8619 | Valid loss: 10.1650, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1829, acc: 0.8650 | Valid loss: 10.2252, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1885, acc: 0.8707 | Valid loss: 10.2302, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1902, acc: 0.8661 | Valid loss: 10.2938, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8648 | Valid loss: 10.3663, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8678 | Valid loss: 10.4076, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1858, acc: 0.8688 | Valid loss: 10.4877, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8682 | Valid loss: 10.5946, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8684 | Valid loss: 10.6380, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1876, acc: 0.8659 | Valid loss: 10.6049, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8671 | Valid loss: 10.5870, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8668 | Valid loss: 10.6049, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1908, acc: 0.8672 | Valid loss: 10.5678, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8683 | Valid loss: 10.5279, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8721 | Valid loss: 10.5245, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1870, acc: 0.8711 | Valid loss: 10.5574, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8695 | Valid loss: 10.5750, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8709 | Valid loss: 10.5788, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8701 | Valid loss: 10.5923, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1986, acc: 0.8674 | Valid loss: 10.6622, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8713 | Valid loss: 10.6849, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1933, acc: 0.8658 | Valid loss: 10.7315, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8655 | Valid loss: 10.7190, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1905, acc: 0.8629 | Valid loss: 10.7007, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8691 | Valid loss: 10.6506, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8683 | Valid loss: 10.6349, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1952, acc: 0.8676 | Valid loss: 10.6228, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8718 | Valid loss: 10.7408, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1809, acc: 0.8718 | Valid loss: 10.7931, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1917, acc: 0.8680 | Valid loss: 10.7434, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8676 | Valid loss: 10.7274, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8717 | Valid loss: 10.7141, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8726 | Valid loss: 10.6552, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2045, acc: 0.8660 | Valid loss: 10.7504, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2117, acc: 0.8522 | Valid loss: 10.6606, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2076, acc: 0.8528 | Valid loss: 10.4862, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2129, acc: 0.8562 | Valid loss: 10.2950, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2077, acc: 0.8660 | Valid loss: 10.2175, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8679 | Valid loss: 10.2293, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1993, acc: 0.8697 | Valid loss: 10.3019, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1995, acc: 0.8671 | Valid loss: 10.2919, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2042, acc: 0.8623 | Valid loss: 10.2330, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2034, acc: 0.8603 | Valid loss: 10.2122, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2076, acc: 0.8607 | Valid loss: 10.1542, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1985, acc: 0.8603 | Valid loss: 10.1265, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2068, acc: 0.8615 | Valid loss: 10.1039, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1965, acc: 0.8648 | Valid loss: 10.1490, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1898, acc: 0.8626 | Valid loss: 10.1537, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2074, acc: 0.8605 | Valid loss: 10.0836, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2222, acc: 0.8512 | Valid loss: 10.0540, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2404, acc: 0.8474 | Valid loss: 9.9665, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2150, acc: 0.8511 | Valid loss: 9.5560, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2414, acc: 0.8525 | Valid loss: 9.2042, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2099, acc: 0.8506 | Valid loss: 9.0308, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2140, acc: 0.8530 | Valid loss: 8.9779, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2008, acc: 0.8594 | Valid loss: 9.0494, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2052, acc: 0.8575 | Valid loss: 9.1731, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8624 | Valid loss: 9.2880, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2126, acc: 0.8607 | Valid loss: 9.3969, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8636 | Valid loss: 9.4970, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8620 | Valid loss: 9.5906, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1999, acc: 0.8584 | Valid loss: 9.6543, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1920, acc: 0.8619 | Valid loss: 9.6601, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1905, acc: 0.8648 | Valid loss: 9.6891, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1932, acc: 0.8683 | Valid loss: 9.7372, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8677 | Valid loss: 9.7804, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8680 | Valid loss: 9.8205, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8669 | Valid loss: 9.8897, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1947, acc: 0.8643 | Valid loss: 9.8283, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2086, acc: 0.8670 | Valid loss: 9.7907, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1937, acc: 0.8648 | Valid loss: 9.8444, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2004, acc: 0.8656 | Valid loss: 9.8645, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8645 | Valid loss: 9.8933, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2031, acc: 0.8639 | Valid loss: 9.9428, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8615 | Valid loss: 10.0197, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8635 | Valid loss: 9.9779, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1892, acc: 0.8675 | Valid loss: 9.9301, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8692 | Valid loss: 9.9258, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8708 | Valid loss: 9.9262, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8695 | Valid loss: 9.9636, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8688 | Valid loss: 10.0886, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1917, acc: 0.8686 | Valid loss: 10.1828, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1861, acc: 0.8683 | Valid loss: 10.2672, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8664 | Valid loss: 10.2933, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1872, acc: 0.8679 | Valid loss: 10.3127, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2011, acc: 0.8683 | Valid loss: 10.3543, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1806, acc: 0.8723 | Valid loss: 10.3718, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1793, acc: 0.8689 | Valid loss: 10.3912, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8701 | Valid loss: 10.3782, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8684 | Valid loss: 10.4119, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1957, acc: 0.8639 | Valid loss: 10.3875, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1890, acc: 0.8648 | Valid loss: 10.2829, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1849, acc: 0.8696 | Valid loss: 10.2561, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1904, acc: 0.8704 | Valid loss: 10.2826, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8701 | Valid loss: 10.3213, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8698 | Valid loss: 10.3535, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8669 | Valid loss: 10.3689, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8707 | Valid loss: 10.3720, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1841, acc: 0.8738 | Valid loss: 10.3930, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8696 | Valid loss: 10.4288, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1830, acc: 0.8718 | Valid loss: 10.4587, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8663 | Valid loss: 10.4582, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1902, acc: 0.8704 | Valid loss: 10.4617, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1842, acc: 0.8707 | Valid loss: 10.4866, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1876, acc: 0.8707 | Valid loss: 10.5055, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1836, acc: 0.8726 | Valid loss: 10.5492, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8677 | Valid loss: 10.6490, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1887, acc: 0.8679 | Valid loss: 10.6449, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1916, acc: 0.8663 | Valid loss: 10.6016, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1855, acc: 0.8680 | Valid loss: 10.5627, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8698 | Valid loss: 10.4606, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1916, acc: 0.8724 | Valid loss: 10.4173, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8733 | Valid loss: 10.4085, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1849, acc: 0.8682 | Valid loss: 10.3786, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1908, acc: 0.8670 | Valid loss: 10.3376, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8704 | Valid loss: 10.3918, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2057, acc: 0.8658 | Valid loss: 10.4288, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1894, acc: 0.8645 | Valid loss: 10.4491, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1873, acc: 0.8626 | Valid loss: 10.3321, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1902, acc: 0.8663 | Valid loss: 10.2472, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1822, acc: 0.8714 | Valid loss: 10.2583, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8733 | Valid loss: 10.2724, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1865, acc: 0.8765 | Valid loss: 10.3580, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8701 | Valid loss: 10.4650, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2060, acc: 0.8611 | Valid loss: 9.9592, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2354, acc: 0.8462 | Valid loss: 9.4768, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2368, acc: 0.8417 | Valid loss: 9.2888, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2109, acc: 0.8481 | Valid loss: 9.2451, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1994, acc: 0.8537 | Valid loss: 9.3288, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2163, acc: 0.8544 | Valid loss: 9.3949, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2026, acc: 0.8633 | Valid loss: 9.4167, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1942, acc: 0.8630 | Valid loss: 9.5258, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8694 | Valid loss: 9.6854, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1896, acc: 0.8662 | Valid loss: 9.8334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8653 | Valid loss: 9.9456, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8627 | Valid loss: 10.0015, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1937, acc: 0.8634 | Valid loss: 9.9993, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8659 | Valid loss: 10.0027, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1961, acc: 0.8615 | Valid loss: 10.0623, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8643 | Valid loss: 10.1157, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1905, acc: 0.8667 | Valid loss: 10.0934, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8681 | Valid loss: 10.0450, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1918, acc: 0.8687 | Valid loss: 10.0953, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1984, acc: 0.8701 | Valid loss: 9.9738, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1999, acc: 0.8671 | Valid loss: 9.9843, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8643 | Valid loss: 10.0348, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8646 | Valid loss: 10.0748, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1916, acc: 0.8630 | Valid loss: 10.1332, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1884, acc: 0.8641 | Valid loss: 10.1810, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1934, acc: 0.8661 | Valid loss: 10.1762, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1980, acc: 0.8688 | Valid loss: 10.1949, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1866, acc: 0.8701 | Valid loss: 10.1650, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1954, acc: 0.8674 | Valid loss: 10.2348, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8671 | Valid loss: 10.1904, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1837, acc: 0.8674 | Valid loss: 10.1605, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1876, acc: 0.8657 | Valid loss: 10.1491, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8656 | Valid loss: 10.1706, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1920, acc: 0.8685 | Valid loss: 10.2051, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8684 | Valid loss: 10.2864, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8650 | Valid loss: 10.4026, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8618 | Valid loss: 10.4617, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1936, acc: 0.8636 | Valid loss: 10.4006, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8688 | Valid loss: 10.2865, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1877, acc: 0.8711 | Valid loss: 10.2370, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1907, acc: 0.8691 | Valid loss: 10.2493, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8651 | Valid loss: 10.2842, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1905, acc: 0.8686 | Valid loss: 10.3109, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1960, acc: 0.8689 | Valid loss: 10.3996, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1865, acc: 0.8682 | Valid loss: 10.5359, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2044, acc: 0.8651 | Valid loss: 10.5811, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8651 | Valid loss: 10.5689, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8671 | Valid loss: 10.5269, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1925, acc: 0.8661 | Valid loss: 10.5146, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8717 | Valid loss: 10.4382, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1875, acc: 0.8723 | Valid loss: 10.3531, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1832, acc: 0.8738 | Valid loss: 10.2529, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8733 | Valid loss: 10.2839, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8687 | Valid loss: 10.3973, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8696 | Valid loss: 10.4874, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1878, acc: 0.8681 | Valid loss: 10.5465, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1884, acc: 0.8677 | Valid loss: 10.5724, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8695 | Valid loss: 10.5415, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8668 | Valid loss: 10.4568, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8672 | Valid loss: 10.3533, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2007, acc: 0.8669 | Valid loss: 10.3546, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1925, acc: 0.8677 | Valid loss: 10.3936, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8722 | Valid loss: 10.4296, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1907, acc: 0.8729 | Valid loss: 10.4939, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8719 | Valid loss: 10.5442, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1873, acc: 0.8692 | Valid loss: 10.5906, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8668 | Valid loss: 10.5907, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1871, acc: 0.8666 | Valid loss: 10.5989, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1977, acc: 0.8687 | Valid loss: 10.6378, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1894, acc: 0.8730 | Valid loss: 10.6889, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8655 | Valid loss: 10.8571, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1882, acc: 0.8669 | Valid loss: 10.7219, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1959, acc: 0.8673 | Valid loss: 10.6626, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8668 | Valid loss: 10.6167, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1822, acc: 0.8710 | Valid loss: 10.6211, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1966, acc: 0.8713 | Valid loss: 10.6379, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2010, acc: 0.8713 | Valid loss: 10.6632, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1951, acc: 0.8656 | Valid loss: 10.7138, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8651 | Valid loss: 10.7649, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1896, acc: 0.8656 | Valid loss: 10.7042, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1993, acc: 0.8666 | Valid loss: 10.6001, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8710 | Valid loss: 10.6173, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1824, acc: 0.8717 | Valid loss: 10.6013, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1815, acc: 0.8707 | Valid loss: 10.5831, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1875, acc: 0.8736 | Valid loss: 10.5642, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1834, acc: 0.8719 | Valid loss: 10.6189, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1923, acc: 0.8696 | Valid loss: 10.6205, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8687 | Valid loss: 10.5809, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1822, acc: 0.8688 | Valid loss: 10.5843, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1973, acc: 0.8703 | Valid loss: 10.5279, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1826, acc: 0.8685 | Valid loss: 10.5213, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1863, acc: 0.8720 | Valid loss: 10.5708, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1940, acc: 0.8722 | Valid loss: 10.6120, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8688 | Valid loss: 10.6575, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2009, acc: 0.8726 | Valid loss: 10.7084, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1907, acc: 0.8672 | Valid loss: 10.7463, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1816, acc: 0.8662 | Valid loss: 10.5870, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1949, acc: 0.8701 | Valid loss: 10.4960, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1783, acc: 0.8729 | Valid loss: 10.5385, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1856, acc: 0.8733 | Valid loss: 10.5877, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8755 | Valid loss: 10.6445, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8738 | Valid loss: 10.7345, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1829, acc: 0.8729 | Valid loss: 10.7950, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1894, acc: 0.8658 | Valid loss: 10.7975, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1805, acc: 0.8700 | Valid loss: 10.8124, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1883, acc: 0.8715 | Valid loss: 10.8055, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1992, acc: 0.8762 | Valid loss: 10.8204, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8725 | Valid loss: 10.8529, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1939, acc: 0.8711 | Valid loss: 10.8804, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1817, acc: 0.8701 | Valid loss: 10.8775, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8676 | Valid loss: 10.8399, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1837, acc: 0.8738 | Valid loss: 10.8491, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1858, acc: 0.8721 | Valid loss: 10.8463, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1834, acc: 0.8722 | Valid loss: 10.8452, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1898, acc: 0.8707 | Valid loss: 10.8486, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1897, acc: 0.8693 | Valid loss: 10.8563, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1797, acc: 0.8733 | Valid loss: 10.8456, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1785, acc: 0.8722 | Valid loss: 10.8123, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1831, acc: 0.8755 | Valid loss: 10.8311, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1878, acc: 0.8795 | Valid loss: 10.9108, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1875, acc: 0.8755 | Valid loss: 10.9640, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8675 | Valid loss: 10.8547, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8715 | Valid loss: 10.5384, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1914, acc: 0.8749 | Valid loss: 10.4266, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1823, acc: 0.8750 | Valid loss: 10.5942, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1928, acc: 0.8717 | Valid loss: 10.6801, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1833, acc: 0.8681 | Valid loss: 10.7277, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8670 | Valid loss: 10.7058, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1861, acc: 0.8727 | Valid loss: 10.6814, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1752, acc: 0.8727 | Valid loss: 10.7103, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1788, acc: 0.8695 | Valid loss: 10.7410, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1879, acc: 0.8734 | Valid loss: 10.7864, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1866, acc: 0.8738 | Valid loss: 10.8664, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1923, acc: 0.8718 | Valid loss: 10.8969, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1818, acc: 0.8742 | Valid loss: 10.9117, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8738 | Valid loss: 10.9446, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1820, acc: 0.8713 | Valid loss: 10.9138, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1839, acc: 0.8714 | Valid loss: 10.8746, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1800, acc: 0.8715 | Valid loss: 10.7931, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1792, acc: 0.8747 | Valid loss: 10.7770, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8787 | Valid loss: 10.8208, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1993, acc: 0.8736 | Valid loss: 10.9010, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2089, acc: 0.8594 | Valid loss: 10.9160, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2112, acc: 0.8580 | Valid loss: 10.8422, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2046, acc: 0.8610 | Valid loss: 10.8344, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1946, acc: 0.8663 | Valid loss: 10.7355, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8719 | Valid loss: 10.6733, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8679 | Valid loss: 10.6592, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1915, acc: 0.8677 | Valid loss: 10.6184, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1975, acc: 0.8682 | Valid loss: 10.5497, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1835, acc: 0.8754 | Valid loss: 10.5606, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1836, acc: 0.8711 | Valid loss: 10.5791, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8705 | Valid loss: 10.6536, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1937, acc: 0.8696 | Valid loss: 10.5620, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1963, acc: 0.8698 | Valid loss: 10.5863, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8687 | Valid loss: 10.6363, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1814, acc: 0.8716 | Valid loss: 10.6518, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1880, acc: 0.8690 | Valid loss: 10.6740, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1916, acc: 0.8722 | Valid loss: 10.7217, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8699 | Valid loss: 10.7250, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1927, acc: 0.8658 | Valid loss: 10.7437, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2024, acc: 0.8671 | Valid loss: 10.7419, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1946, acc: 0.8672 | Valid loss: 10.7593, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8699 | Valid loss: 10.7488, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1947, acc: 0.8697 | Valid loss: 10.6386, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8752 | Valid loss: 10.6519, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8739 | Valid loss: 10.6899, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1894, acc: 0.8699 | Valid loss: 10.7280, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1862, acc: 0.8702 | Valid loss: 10.7378, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1843, acc: 0.8704 | Valid loss: 10.7595, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8701 | Valid loss: 10.7715, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8729 | Valid loss: 10.8201, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1815, acc: 0.8730 | Valid loss: 10.8739, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1823, acc: 0.8710 | Valid loss: 10.8932, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1854, acc: 0.8708 | Valid loss: 10.9068, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1862, acc: 0.8751 | Valid loss: 10.9120, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8714 | Valid loss: 10.8784, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1826, acc: 0.8736 | Valid loss: 10.9088, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1915, acc: 0.8735 | Valid loss: 10.9564, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1800, acc: 0.8734 | Valid loss: 10.9832, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1933, acc: 0.8687 | Valid loss: 11.2036, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2084, acc: 0.8549 | Valid loss: 11.2967, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2083, acc: 0.8557 | Valid loss: 11.1941, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1998, acc: 0.8554 | Valid loss: 11.0470, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1940, acc: 0.8625 | Valid loss: 10.8834, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8728 | Valid loss: 10.8362, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1845, acc: 0.8718 | Valid loss: 10.8531, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2010, acc: 0.8576 | Valid loss: 10.5823, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2322, acc: 0.8468 | Valid loss: 10.1070, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2402, acc: 0.8439 | Valid loss: 9.8676, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2199, acc: 0.8528 | Valid loss: 9.7546, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2015, acc: 0.8603 | Valid loss: 9.7035, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2121, acc: 0.8592 | Valid loss: 9.6706, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8657 | Valid loss: 9.6993, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1978, acc: 0.8650 | Valid loss: 9.7594, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1982, acc: 0.8683 | Valid loss: 9.8512, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8645 | Valid loss: 9.9428, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1940, acc: 0.8667 | Valid loss: 10.0468, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8650 | Valid loss: 10.1134, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1870, acc: 0.8693 | Valid loss: 10.1681, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1952, acc: 0.8682 | Valid loss: 10.2313, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1911, acc: 0.8709 | Valid loss: 10.2846, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1952, acc: 0.8696 | Valid loss: 10.3626, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1851, acc: 0.8729 | Valid loss: 10.4654, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1855, acc: 0.8701 | Valid loss: 10.5562, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1891, acc: 0.8687 | Valid loss: 10.6054, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1806, acc: 0.8709 | Valid loss: 10.5594, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1935, acc: 0.8688 | Valid loss: 10.5905, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8697 | Valid loss: 10.6354, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1951, acc: 0.8686 | Valid loss: 10.6970, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1916, acc: 0.8627 | Valid loss: 10.6841, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8677 | Valid loss: 10.6541, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1870, acc: 0.8647 | Valid loss: 10.6308, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1877, acc: 0.8653 | Valid loss: 10.6194, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1850, acc: 0.8706 | Valid loss: 10.6486, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1901, acc: 0.8751 | Valid loss: 10.6925, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1811, acc: 0.8741 | Valid loss: 10.7830, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1887, acc: 0.8748 | Valid loss: 10.8420, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1779, acc: 0.8723 | Valid loss: 10.9433, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1863, acc: 0.8709 | Valid loss: 11.0028, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1902, acc: 0.8683 | Valid loss: 11.0356, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1769, acc: 0.8725 | Valid loss: 11.0425, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1884, acc: 0.8725 | Valid loss: 11.0609, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1841, acc: 0.8724 | Valid loss: 11.1042, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1834, acc: 0.8752 | Valid loss: 11.1210, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1876, acc: 0.8701 | Valid loss: 11.0794, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1852, acc: 0.8720 | Valid loss: 11.0901, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8709 | Valid loss: 11.0602, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1846, acc: 0.8713 | Valid loss: 11.0946, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1845, acc: 0.8709 | Valid loss: 11.0680, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1917, acc: 0.8685 | Valid loss: 11.0145, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1877, acc: 0.8691 | Valid loss: 11.0225, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1927, acc: 0.8687 | Valid loss: 11.0513, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1926, acc: 0.8653 | Valid loss: 11.0454, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8716 | Valid loss: 11.0623, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8679 | Valid loss: 11.0585, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1855, acc: 0.8698 | Valid loss: 11.0252, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1800, acc: 0.8738 | Valid loss: 11.0123, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1893, acc: 0.8691 | Valid loss: 11.0109, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1833, acc: 0.8709 | Valid loss: 10.9961, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1922, acc: 0.8658 | Valid loss: 10.9647, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1932, acc: 0.8671 | Valid loss: 10.7987, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1884, acc: 0.8664 | Valid loss: 10.6946, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1838, acc: 0.8698 | Valid loss: 10.7250, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1900, acc: 0.8717 | Valid loss: 10.8643, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1903, acc: 0.8732 | Valid loss: 10.9338, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2050, acc: 0.8673 | Valid loss: 10.9874, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1924, acc: 0.8635 | Valid loss: 10.9228, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1859, acc: 0.8655 | Valid loss: 10.7762, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1826, acc: 0.8715 | Valid loss: 10.7893, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1826, acc: 0.8693 | Valid loss: 10.8455, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8707 | Valid loss: 10.8737, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1868, acc: 0.8732 | Valid loss: 10.8843, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1919, acc: 0.8704 | Valid loss: 10.6996, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1944, acc: 0.8582 | Valid loss: 10.3719, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8616 | Valid loss: 10.0939, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8626 | Valid loss: 10.0270, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1999, acc: 0.8711 | Valid loss: 10.1134, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8720 | Valid loss: 10.2893, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1955, acc: 0.8671 | Valid loss: 10.4052, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1943, acc: 0.8619 | Valid loss: 10.4869, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2015, acc: 0.8635 | Valid loss: 10.4923, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8700 | Valid loss: 10.4420, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1972, acc: 0.8738 | Valid loss: 10.4480, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8639 | Valid loss: 10.5055, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1987, acc: 0.8642 | Valid loss: 10.6000, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1921, acc: 0.8635 | Valid loss: 10.6292, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1854, acc: 0.8680 | Valid loss: 10.5964, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8747 | Valid loss: 10.5326, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1823, acc: 0.8778 | Valid loss: 10.5578, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8733 | Valid loss: 10.6440, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1917, acc: 0.8677 | Valid loss: 10.7419, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1881, acc: 0.8709 | Valid loss: 10.6731, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1920, acc: 0.8717 | Valid loss: 10.6137, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1896, acc: 0.8722 | Valid loss: 10.6635, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8669 | Valid loss: 10.6865, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8691 | Valid loss: 10.7485, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2003, acc: 0.8697 | Valid loss: 10.7735, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1815, acc: 0.8709 | Valid loss: 10.7727, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1819, acc: 0.8673 | Valid loss: 10.7337, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1857, acc: 0.8741 | Valid loss: 10.7519, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1837, acc: 0.8707 | Valid loss: 10.8048, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1874, acc: 0.8696 | Valid loss: 10.8597, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1966, acc: 0.8742 | Valid loss: 10.8927, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1951, acc: 0.8674 | Valid loss: 10.8385, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8714 | Valid loss: 10.7980, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1890, acc: 0.8715 | Valid loss: 10.7999, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1835, acc: 0.8724 | Valid loss: 10.8093, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8688 | Valid loss: 10.8150, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1899, acc: 0.8709 | Valid loss: 10.8556, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1858, acc: 0.8727 | Valid loss: 10.9370, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1847, acc: 0.8671 | Valid loss: 10.9653, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1915, acc: 0.8718 | Valid loss: 10.9880, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1866, acc: 0.8723 | Valid loss: 11.0309, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8715 | Valid loss: 11.0738, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1856, acc: 0.8681 | Valid loss: 11.0889, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1830, acc: 0.8707 | Valid loss: 11.1075, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1905, acc: 0.8685 | Valid loss: 11.1224, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1902, acc: 0.8699 | Valid loss: 11.0676, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1760, acc: 0.8720 | Valid loss: 10.9470, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8741 | Valid loss: 10.9164, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1818, acc: 0.8717 | Valid loss: 10.9468, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1793, acc: 0.8749 | Valid loss: 11.0168, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1850, acc: 0.8728 | Valid loss: 10.9672, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.2037, acc: 0.8667 | Valid loss: 10.9395, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1957, acc: 0.8661 | Valid loss: 11.0017, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1923, acc: 0.8690 | Valid loss: 11.0171, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1938, acc: 0.8658 | Valid loss: 11.1284, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1964, acc: 0.8642 | Valid loss: 11.0732, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8725 | Valid loss: 10.9979, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1874, acc: 0.8748 | Valid loss: 10.9809, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1796, acc: 0.8739 | Valid loss: 10.9881, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8716 | Valid loss: 11.0474, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1941, acc: 0.8720 | Valid loss: 11.1158, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1849, acc: 0.8720 | Valid loss: 11.1136, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1887, acc: 0.8710 | Valid loss: 11.0849, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1974, acc: 0.8747 | Valid loss: 11.0207, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1890, acc: 0.8744 | Valid loss: 10.9217, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1860, acc: 0.8731 | Valid loss: 10.8889, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1869, acc: 0.8715 | Valid loss: 10.7470, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1997, acc: 0.8685 | Valid loss: 10.7334, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1915, acc: 0.8691 | Valid loss: 10.8015, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1886, acc: 0.8735 | Valid loss: 10.8792, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1907, acc: 0.8682 | Valid loss: 10.9547, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1829, acc: 0.8746 | Valid loss: 10.9957, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1892, acc: 0.8749 | Valid loss: 11.0094, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1857, acc: 0.8739 | Valid loss: 10.9911, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1953, acc: 0.8691 | Valid loss: 11.0325, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1872, acc: 0.8664 | Valid loss: 11.0576, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1785, acc: 0.8684 | Valid loss: 11.0717, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1926, acc: 0.8701 | Valid loss: 11.0818, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1909, acc: 0.8741 | Valid loss: 11.1276, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1867, acc: 0.8754 | Valid loss: 11.2075, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1828, acc: 0.8741 | Valid loss: 11.2567, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1838, acc: 0.8736 | Valid loss: 11.2665, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8674 | Valid loss: 11.1713, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1958, acc: 0.8639 | Valid loss: 11.0593, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1851, acc: 0.8681 | Valid loss: 10.9271, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1930, acc: 0.8725 | Valid loss: 10.8462, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1971, acc: 0.8725 | Valid loss: 10.7957, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1798, acc: 0.8699 | Valid loss: 10.7890, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1910, acc: 0.8696 | Valid loss: 10.8420, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1816, acc: 0.8754 | Valid loss: 10.9349, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1803, acc: 0.8769 | Valid loss: 11.0183, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1900, acc: 0.8736 | Valid loss: 10.9956, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1880, acc: 0.8729 | Valid loss: 10.9209, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1832, acc: 0.8722 | Valid loss: 10.9332, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1790, acc: 0.8725 | Valid loss: 10.9747, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1818, acc: 0.8765 | Valid loss: 11.0973, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1859, acc: 0.8744 | Valid loss: 11.1697, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1799, acc: 0.8755 | Valid loss: 11.2403, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1859, acc: 0.8683 | Valid loss: 11.3569, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1893, acc: 0.8694 | Valid loss: 11.2124, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1948, acc: 0.8659 | Valid loss: 11.1883, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1820, acc: 0.8715 | Valid loss: 11.2243, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1908, acc: 0.8742 | Valid loss: 11.2305, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1880, acc: 0.8747 | Valid loss: 11.2721, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1776, acc: 0.8756 | Valid loss: 11.3573, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1878, acc: 0.8698 | Valid loss: 11.4162, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1929, acc: 0.8702 | Valid loss: 11.4146, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1888, acc: 0.8687 | Valid loss: 11.3758, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1950, acc: 0.8685 | Valid loss: 11.3183, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1786, acc: 0.8716 | Valid loss: 11.1993, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1813, acc: 0.8733 | Valid loss: 11.0940, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1846, acc: 0.8762 | Valid loss: 11.1114, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1851, acc: 0.8735 | Valid loss: 11.1478, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1945, acc: 0.8748 | Valid loss: 11.2204, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1781, acc: 0.8779 | Valid loss: 11.3417, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1906, acc: 0.8752 | Valid loss: 11.4004, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1858, acc: 0.8742 | Valid loss: 11.4123, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1956, acc: 0.8706 | Valid loss: 11.4026, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1846, acc: 0.8730 | Valid loss: 11.4073, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1882, acc: 0.8760 | Valid loss: 11.3681, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1872, acc: 0.8757 | Valid loss: 11.3246, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1858, acc: 0.8729 | Valid loss: 11.2682, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Train loss: 0.1800, acc: 0.8675 | Valid loss: 11.2253, acc: 0.8611 | Test loss: 0.5332, acc: 0.9576\n",
      "Training Completed!!!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam()\n",
    "# loss_fn = SparseCategoricalCrossentropy()\n",
    "# loss_fn = BinaryCrossentropy()\n",
    "loss_fn = binary_crossentropy\n",
    "loader_tr, loader_va, loader_te = load_tr_data(epochs)\n",
    "# def input_mask(target):\n",
    "#     minimum = tf.math.minimum(tf.reduce_sum(tf.dtypes.cast(target == 0, tf.int32)), tf.reduce_sum(tf.dtypes.cast(target == 1, tf.int32)))\n",
    "#     zeroes_index = tf.random.shuffle(tf.where(target == 0)[:,0])[:minimum]\n",
    "#     ones_index = tf.random.shuffle(tf.where(target == 1)[:,0])[:minimum]\n",
    "    \n",
    "#     stacked_inp = tf.reshape(tf.stack([zeroes_index, ones_index]), (-1,1))\n",
    "#     zeros_stack = tf.reshape(tf.stack([stacked_inp, tf.zeros((minimum*2,1), dtype='int64')], axis=1), (-1,2))\n",
    "#     sparsed_data = tf.SparseTensor(zeros_stack, tf.ones((minimum*2, )), target.shape)\n",
    "#     return tf.sparse.to_dense(tf.sparse.reorder(sparsed_data), default_value = 0. )\n",
    "\n",
    "def masked_loss_and_accuracy(target, predictions):\n",
    "#     values_keep = tf.gather( logits[0], idx_keep )\n",
    "    loss = loss_fn(target, predictions)\n",
    "    minimum = tf.math.minimum(tf.reduce_sum(tf.dtypes.cast(target == 0, tf.int32)), tf.reduce_sum(tf.dtypes.cast(target == 1, tf.int32)))\n",
    "    zeroes_index = tf.random.shuffle(tf.where(target == 0)[:,0])\n",
    "    ones_index = tf.random.shuffle(tf.where(target == 1)[:,0])\n",
    "    loss_zeroes =  tf.reduce_mean(tf.gather(loss, zeroes_index))\n",
    "    loss_ones = tf.reduce_mean(tf.gather(loss, ones_index))\n",
    "    \n",
    "    weight_for_0 = tf.cast((1 / tf.shape(zeroes_index)[0])*(tf.cast(tf.shape(target)[0], 'float64'))/2.0, 'float32')\n",
    "    weight_for_1 = tf.cast((1 / tf.shape(ones_index)[0])*(tf.cast(tf.shape(target)[0], 'float64'))/2.0, 'float32')\n",
    "\n",
    "#     pdb.set_trace()\n",
    "    loss = (weight_for_0 * loss_zeroes * 0.6 +  weight_for_1 * loss_ones * 0.4)\n",
    "    \n",
    "#     loss = tf.reduce_mean(tf.multiply(loss, mask))\n",
    "    bin_acc = binary_accuracy(target, predictions)\n",
    "    acc = tf.reduce_mean(bin_acc)\n",
    "    return loss, acc\n",
    "\n",
    "# Training function\n",
    "@tf.function\n",
    "def train_on_batch(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs[:-1], training=True)\n",
    "        loss, acc = masked_loss_and_accuracy(target, predictions)\n",
    "        loss = loss + sum(model.losses)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    step = 0\n",
    "    results = []\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        inputs, target = batch\n",
    "        predictions = model(inputs[:-1], training=False)\n",
    "        loss, acc = masked_loss_and_accuracy(target, predictions)\n",
    "        results.append((loss, acc, len(target)))  # Keep track of batch size\n",
    "        if step == loader.steps_per_epoch:\n",
    "            results = np.array(results)\n",
    "            return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
    "\n",
    "patience = 10\n",
    "# Setup training\n",
    "best_val_loss = 99999\n",
    "current_patience = patience\n",
    "step = 0\n",
    "\n",
    "# Training loop\n",
    "results_tr = []\n",
    "history = []\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "\n",
    "    # Training step\n",
    "    inputs, target = batch\n",
    "    loss, acc = train_on_batch(inputs, target)\n",
    "    results_tr.append((loss, acc, len(target)))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        results_va = evaluate(loader_va)\n",
    "        if results_va[0] < best_val_loss:\n",
    "            best_val_loss = results_va[0]\n",
    "            current_patience = patience\n",
    "            results_te = evaluate(loader_te)\n",
    "        else:\n",
    "            current_patience -= 1\n",
    "            if current_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "#                 break\n",
    "\n",
    "        # Print results\n",
    "        results_tr = np.array(results_tr)\n",
    "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
    "        print(\n",
    "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Valid loss: {:.4f}, acc: {:.4f} | \"\n",
    "            \"Test loss: {:.4f}, acc: {:.4f}\".format(\n",
    "                *results_tr, *results_va, *results_te\n",
    "            )\n",
    "        )\n",
    "        history.append([results_tr[0], results_va[0]])\n",
    "        # Reset epoch\n",
    "        results_tr = []\n",
    "        step = 0\n",
    "        \n",
    "print(\"Training Completed!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5ba5cac50>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss_history = list(map(lambda x: x.numpy(), history))\n",
    "# # loss_history\n",
    "df = pd.DataFrame(history)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbilkFE24XSD",
    "outputId": "f1c78ac0-5b27-44da-a58d-760922064f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd5bb9617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "GCN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       449\n",
      "         1.0       0.89      1.00      0.94        50\n",
      "\n",
      "    accuracy                           0.99       499\n",
      "   macro avg       0.95      0.99      0.97       499\n",
      "weighted avg       0.99      0.99      0.99       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_te = dataset[4].x\n",
    "A_te = dataset[4].a\n",
    "y_te = dataset[4].y\n",
    "\n",
    "y_pred = model.predict([X_te, A_te], batch_size=X_te.shape[0])\n",
    "report = classification_report(y_te, (y_pred >= 0.5).astype(int))\n",
    "print('GCN Classification Report: \\n {}'.format(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd5bb9617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "GCN Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.97      0.97      3134\n",
      "         1.0       0.76      0.86      0.81       406\n",
      "\n",
      "    accuracy                           0.95      3540\n",
      "   macro avg       0.87      0.91      0.89      3540\n",
      "weighted avg       0.96      0.95      0.95      3540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_te = dataset[8].x\n",
    "A_te = dataset[8].a\n",
    "y_te = dataset[8].y\n",
    "\n",
    "y_pred = model.predict([X_te, A_te], batch_size=X_te.shape[0])\n",
    "report = classification_report(y_te, (y_pred >= 0.5).astype(int))\n",
    "print('GCN Classification Report: \\n {}'.format(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nxKznEH4ltT",
    "outputId": "e9cdb608-825d-4611-b2c8-f60932e3a0ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred >= 0.5).astype(int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = tf.keras.metrics.binary_accuracy(y_te, y_pred).numpy()\n",
    "minimum = np.array([(y_te == 0).sum(), (y_te == 1).sum()]).min()\n",
    "zeroes_index = np.random.choice(np.where(y_te == 0)[0], minimum, replace=False)\n",
    "ones_index = np.random.choice(np.where(y_te == 1)[0], minimum, replace=False)\n",
    "mask = np.zeros_like(y_te)\n",
    "mask[zeroes_index] = 1\n",
    "mask[ones_index] = 1\n",
    "# np.multiply(bin_acc, mask).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.convert_to_tensor(y_te)\n",
    "minimum = tf.math.minimum(tf.reduce_sum(tf.dtypes.cast(target == 0, tf.int32)), tf.reduce_sum(tf.dtypes.cast(target == 1, tf.int32)))\n",
    "zeroes_index = tf.random.shuffle(tf.where(target == 0)[:,0])[:minimum]\n",
    "ones_index = tf.random.shuffle(tf.where(target == 1)[:,0])[:minimum]\n",
    "mask = tf.Variable(tf.zeros_like(target))\n",
    "stacked_inp = tf.reshape(tf.stack([zeroes_index, ones_index]), (-1,1))\n",
    "zeros_stack = tf.reshape(tf.stack([stacked_inp, tf.zeros((minimum*2,1), dtype='int64')], axis=1), (-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.convert_to_tensor(y_te)\n",
    "minimum = tf.math.minimum(tf.reduce_sum(tf.dtypes.cast(target == 0, tf.int32)), tf.reduce_sum(tf.dtypes.cast(target == 1, tf.int32)))\n",
    "zeroes_index = tf.random.shuffle(tf.where(target == 0))[:minimum, :]\n",
    "ones_index = tf.random.shuffle(tf.where(target == 1))[:minimum, :]\n",
    "mask = tf.Variable(tf.zeros_like(target))\n",
    "stacked_inp = tf.stack([zeroes_index, ones_index], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([50]),\n",
       " TensorShape([50]),\n",
       " TensorShape([100, 1]),\n",
       " TensorShape([100]),\n",
       " TensorShape([499, 1]),\n",
       " TensorShape([100, 2]))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.SparseTensor(stacked_inp, tf.ones((minimum*2, )), target.shape)\n",
    "zeroes_index.shape, ones_index.shape, stacked_inp.shape, tf.ones((minimum*2, )).shape, target.shape, zeros_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsed_data = tf.SparseTensor(zeros_stack, tf.ones((minimum*2, )), target.shape)\n",
    "mask = tf.sparse.to_dense(tf.sparse.reorder(sparsed_data), default_value = 0. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=100.0>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(densed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filtering( logits, top_k = 5):\n",
    "    \n",
    "    # a[...,1] equivalent to a[: ,: ,1 ]\n",
    "    indices_to_remove = logits < tf.math.top_k(logits,top_k)[0][..., -1, None]\n",
    "    # indices_to_remove is a tensor of bool values e.g. [ True, False, False, ..., True ]\n",
    "\n",
    "    # 1d indices\n",
    "    idx_remove = tf.where( indices_to_remove == 0 )[:,-1]\n",
    "    idx_keep = tf.where( indices_to_remove == 1 )[:,-1]\n",
    "    \n",
    "    values_remove = tf.tile( [-float('inf')], [tf.shape(idx_remove)[0]] ) \n",
    "    values_keep = tf.gather( logits[0], idx_keep )\n",
    "\n",
    "    # to create a sparse vector we still need 2d indices like [ [0,1], [0,2], [0,10] ]\n",
    "    # create vectors of 0's that we'll later stack with the actual indices\n",
    "    zeros_remove = tf.zeros_like(idx_remove)\n",
    "    zeros_keep = tf.zeros_like(idx_keep)\n",
    "\n",
    "    idx_remove = tf.stack( [ zeros_remove, idx_remove], axis=1 )\n",
    "    idx_keep = tf.stack( [ zeros_keep, idx_keep], axis=1 )\n",
    "\n",
    "    # now we can create a sparse matrix\n",
    "    logits_remove = tf.SparseTensor( idx_remove, values_remove, tf.shape(logits, out_type = tf.int64))\n",
    "    logits_keep = tf.SparseTensor( idx_keep, values_keep, tf.shape(logits, out_type = tf.int64))\n",
    "\n",
    "    # add together the two matrices (need to convert them to dense first)\n",
    "    filtered_logits = tf.add(\n",
    "        tf.sparse.to_dense(logits_remove, default_value = 0. ),\n",
    "        tf.sparse.to_dense(logits_keep, default_value = 0. )\n",
    "    )\n",
    "\n",
    "    return filtered_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 432), dtype=bool, numpy=\n",
       "array([[ True, False, False, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True, False, ..., False, False, False]])>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask([1, 3, 2], target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smipV5W2jGKf",
    "outputId": "8a06bd66-4448-46b1-b227-3c8e0453f2de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd5be3ae4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# layer_outputs = [layer.output for layer in model.layers]\n",
    "# activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "# activations = activation_model.predict([X_te, A_te],batch_size=X_te.shape[0])\n",
    "\n",
    "# #Get t-SNE Representation\n",
    "# #get the hidden layer representation after the first GCN layer\n",
    "# x_tsne = TSNE(n_components=2).fit_transform(activations[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "0F-uDNB4jKI8"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def plot_tSNE(labels_encoded,x_tsne):\n",
    "#     color_map = np.argmax(labels_encoded, axis=1)\n",
    "#     plt.figure(figsize=(10,10))\n",
    "#     for cl in range(1):\n",
    "#         indices = np.where(color_map==cl)\n",
    "#         indices = indices[0]\n",
    "#         plt.scatter(x_tsne[indices,0], x_tsne[indices, 1], label=cl)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_tSNE(y_te,x_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upDQ-QBb4m89",
    "outputId": "b5b794de-6b38-4e19-a8d3-1c063d008e75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_true = [[1], [1], [0], [0]]\n",
    "# y_pred = [[1], [1], [0], [0]]\n",
    "tf.keras.metrics.binary_accuracy(y_te, y_pred).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tj8IKy53HxWx",
    "outputId": "b81aa017-ca2a-4fb4-80ed-fe2a2c125c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_te == 0).sum(), (y_te == 1).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCGZgvB1Kg2m",
    "outputId": "4a7d2f7d-1d07-43d2-e772-0f0d70963f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "nVfU7bmKKvkz"
   },
   "outputs": [],
   "source": [
    "y_true = [[1], [1], [0], [0]]\n",
    "y_pred = [[0.8], [0.8], [0.3], [0.3]]\n",
    "m = tf.keras.metrics.binary_accuracy(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfSffWD9MANQ",
    "outputId": "87c348ae-42d1-4c7f-bf6e-58c15df7951a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36716142],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36708796],\n",
       "       [0.36763102],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.36541492],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.37839413],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.3813891 ],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.38834843],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.39287794],\n",
       "       [0.40640447],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.4082654 ],\n",
       "       [0.4205025 ],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.39123335],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4269486 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4100087 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.4114372 ],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.37812722],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.25657254],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.25657254],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.25657254],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.25657254],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.25657254],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.25657254],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.25657254],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.31654048],\n",
       "       [0.3169052 ],\n",
       "       [0.25657254],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.2921846 ],\n",
       "       [0.12362692],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.41709712],\n",
       "       [0.20426184],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.38555086],\n",
       "       [0.26603675],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.3670572 ],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.29757047],\n",
       "       [0.27216393],\n",
       "       [0.27216393],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.05739915],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ],\n",
       "       [0.4045186 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NT6z43I9MA2k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DisjoingGraphs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
