{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../lib')\n",
    "from data_pre_processing import load_data\n",
    "sys.path.remove('../lib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6288, 6288)\n",
      "(6288, 4)\n"
     ]
    }
   ],
   "source": [
    "A, X, Y = load_data('c6288', '../data/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6288,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y = np.array(Y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras_dgl.layers import GraphAttentionCNN\n",
    "# from keras_dgl.layers import GraphCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh, ArpackNoConvergence\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    # features = normalize_features(features)\n",
    "    # adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features.todense(), adj, labels\n",
    "\n",
    "\n",
    "def load_data_attention(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "\n",
    "    return features.todense(), adj, labels\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def normalize_adj(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
    "    else:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj).tocsr()\n",
    "    return a_norm\n",
    "\n",
    "\n",
    "def normalize_adj_numpy(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d)\n",
    "    else:\n",
    "        d = np.diag(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = d.dot(adj)\n",
    "    return a_norm\n",
    "\n",
    "\n",
    "def preprocess_adj(adj, symmetric=True):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj, symmetric)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def preprocess_adj_numpy(adj, symmetric=True):\n",
    "    adj = adj + np.eye(adj.shape[0])\n",
    "    adj = normalize_adj_numpy(adj, symmetric)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def preprocess_adj_tensor(adj_tensor, symmetric=True):\n",
    "    adj_out_tensor = []\n",
    "    for i in range(adj_tensor.shape[0]):\n",
    "        adj = adj_tensor[i]\n",
    "        adj = adj + np.eye(adj.shape[0])\n",
    "        adj = normalize_adj_numpy(adj, symmetric)\n",
    "        adj_out_tensor.append(adj)\n",
    "    adj_out_tensor = np.array(adj_out_tensor)\n",
    "    return adj_out_tensor\n",
    "\n",
    "\n",
    "def preprocess_adj_tensor_with_identity(adj_tensor, symmetric=True):\n",
    "    adj_out_tensor = []\n",
    "    for i in range(adj_tensor.shape[0]):\n",
    "        adj = adj_tensor[i]\n",
    "        adj = adj + np.eye(adj.shape[0])\n",
    "        adj = normalize_adj_numpy(adj, symmetric)\n",
    "        adj = np.concatenate([np.eye(adj.shape[0]), adj], axis=0)\n",
    "        adj_out_tensor.append(adj)\n",
    "    adj_out_tensor = np.array(adj_out_tensor)\n",
    "    return adj_out_tensor\n",
    "\n",
    "\n",
    "def preprocess_adj_tensor_with_identity_concat(adj_tensor, symmetric=True):\n",
    "    adj_out_tensor = []\n",
    "    for i in range(adj_tensor.shape[0]):\n",
    "        adj = adj_tensor[i]\n",
    "        adj = adj + np.eye(adj.shape[0])\n",
    "        adj = normalize_adj_numpy(adj, symmetric)\n",
    "        adj = np.concatenate([np.eye(adj.shape[0]), adj], axis=0)\n",
    "        adj_out_tensor.append(adj)\n",
    "    adj_out_tensor = np.concatenate(adj_out_tensor, axis=0)\n",
    "    return adj_out_tensor\n",
    "\n",
    "def preprocess_adj_tensor_concat(adj_tensor, symmetric=True):\n",
    "    adj_out_tensor = []\n",
    "    for i in range(adj_tensor.shape[0]):\n",
    "        adj = adj_tensor[i]\n",
    "        adj = adj + np.eye(adj.shape[0])\n",
    "        adj = normalize_adj_numpy(adj, symmetric)\n",
    "        adj_out_tensor.append(adj)\n",
    "    adj_out_tensor = np.concatenate(adj_out_tensor, axis=0)\n",
    "    return adj_out_tensor\n",
    "\n",
    "def preprocess_edge_adj_tensor(edge_adj_tensor, symmetric=True):\n",
    "    edge_adj_out_tensor = []\n",
    "    num_edge_features = int(edge_adj_tensor.shape[1]/edge_adj_tensor.shape[2])\n",
    "\n",
    "    for i in range(edge_adj_tensor.shape[0]):\n",
    "        edge_adj = edge_adj_tensor[i]\n",
    "        edge_adj = np.split(edge_adj, num_edge_features, axis=0)\n",
    "        edge_adj = np.array(edge_adj)\n",
    "        edge_adj = preprocess_adj_tensor_concat(edge_adj, symmetric)\n",
    "        edge_adj_out_tensor.append(edge_adj)\n",
    "\n",
    "    edge_adj_out_tensor = np.array(edge_adj_out_tensor)\n",
    "    return edge_adj_out_tensor\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def get_splits(y):\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "\n",
    "def get_splits_v2(y):\n",
    "    idx_train = range(1708)\n",
    "    idx_val = range(1708, 1708 + 500)\n",
    "    idx_test = range(1708 + 500, 2708)\n",
    "    \n",
    "    y_train = np.zeros(len(y), dtype=np.int32)\n",
    "    y_val = np.zeros(len(y), dtype=np.int32)\n",
    "    y_test = np.zeros(len(y), dtype=np.int32)\n",
    "    \n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
    "\n",
    "\n",
    "def categorical_crossentropy(preds, labels):\n",
    "    return np.mean(-np.log(np.extract(labels, preds)))\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "\n",
    "def evaluate_preds(preds, labels, indices):\n",
    "    split_loss = list()\n",
    "    split_acc = list()\n",
    "\n",
    "    for y_split, idx_split in zip(labels, indices):\n",
    "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "\n",
    "    return split_loss, split_acc\n",
    "\n",
    "\n",
    "def normalized_laplacian(adj, symmetric=True):\n",
    "    adj_normalized = normalize_adj(adj, symmetric)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    return laplacian\n",
    "\n",
    "\n",
    "def rescale_laplacian(laplacian):\n",
    "    try:\n",
    "        print('Calculating largest eigenvalue of normalized graph Laplacian...')\n",
    "        largest_eigval = eigsh(laplacian, 1, which='LM', return_eigenvectors=False)[0]\n",
    "    except ArpackNoConvergence:\n",
    "        print('Eigenvalue calculation did not converge! Using largest_eigval=2 instead.')\n",
    "        largest_eigval = 2\n",
    "\n",
    "    scaled_laplacian = (2. / largest_eigval) * laplacian - sp.eye(laplacian.shape[0])\n",
    "    return scaled_laplacian\n",
    "\n",
    "\n",
    "def chebyshev_polynomial(X, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices.\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    T_k = list()\n",
    "    T_k.append(sp.eye(X.shape[0]).tocsr())\n",
    "    T_k.append(X)\n",
    "\n",
    "    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):\n",
    "        X_ = sp.csr_matrix(X, copy=True)\n",
    "        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two\n",
    "\n",
    "    for i in range(2, k + 1):\n",
    "        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))\n",
    "\n",
    "    return T_k\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Y_val, _, train_idx, val_idx, test_idx, train_mask = get_splits(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.array(train_idx)\n",
    "val_idx = np.array(val_idx)\n",
    "test_idx = np.array(test_idx)\n",
    "labels = Y\n",
    "\n",
    "Y_train = np.zeros(Y.shape)\n",
    "labels_train = np.zeros(labels.shape)\n",
    "Y_train[train_idx] = Y[train_idx]\n",
    "labels_train[train_idx] = labels[train_idx]\n",
    "\n",
    "Y_test = np.zeros(Y.shape)\n",
    "labels_test = np.zeros(labels.shape)\n",
    "Y_test[test_idx] = Y[test_idx]\n",
    "labels_test[test_idx] = labels[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = A*A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6288, 6288)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aa7e8dc048cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# build graph_conv_filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgraph_conv_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgraph_conv_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_conv_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "# build graph_conv_filters\n",
    "num_filters = 3\n",
    "graph_conv_filters = np.concatenate([A, X, A])\n",
    "graph_conv_filters = K.constant(graph_conv_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.6, input_shape=(X.shape[1],)))\n",
    "model.add(GraphAttentionCNN(8, A, num_filters, graph_conv_filters, num_attention_heads=8, attention_combine='concat', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(GraphAttentionCNN(Y.shape[1], A, num_filters, graph_conv_filters, num_attention_heads=1, attention_combine='average', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.005), metrics=['accuracy'])\n",
    "\n",
    "NB_EPOCH = 1000\n",
    "\n",
    "for epoch in range(1, NB_EPOCH + 1):\n",
    "    model.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "    Y_pred = model.predict(X, batch_size=A.shape[0])\n",
    "    _, train_acc = evaluate_preds(Y_pred, [Y_train], [train_idx])\n",
    "    _, test_acc = evaluate_preds(Y_pred, [Y_test], [test_idx])\n",
    "    print(\"Epoch: {:04d}\".format(epoch), \"train_acc= {:.4f}\".format(train_acc[0]), \"test_acc= {:.4f}\".format(test_acc[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
